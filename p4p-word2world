"""
STAP v3.0: Full Custom Implementation
Semantic Topology-Aware Projection - Complete Framework

This implements the complete STAP framework as described in the mathematical whitepaper:
- Cross-entropy optimization with attractive and repulsive forces
- Locally adaptive scaling (σ_i) for varying neighborhood densities
- Heavy-tailed probability distribution in low-dimensional space
- Stochastic gradient descent with negative sampling
- HNSW integration for O(log n) nearest neighbor search

Authors: Shu Bai, Xiaoyue Cao, Jinxing Chen, Yunzhou Dai, Vieux Valcin, Huiyi Zhang
Date: October 18, 2025
"""

import numpy as np
from sentence_transformers import SentenceTransformer
from typing import Dict, List, Tuple, Optional
import logging
from dataclasses import dataclass
import hnswlib
from sklearn.neighbors import NearestNeighbors

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class STAPConfig:
    """Configuration for STAP processor"""
    embedding_model_name: str = 'all-MiniLM-L6-v2'
    target_dim: int = 32
    n_neighbors: int = 15
    min_dist: float = 0.1
    learning_rate: float = 1.0
    n_epochs: int = 200
    negative_sample_rate: int = 5
    a: float = 1.577  # Heavy-tailed distribution parameter
    b: float = 0.895  # Heavy-tailed distribution parameter
    text_weight: float = 0.7
    engagement_weight: float = 0.3
    random_state: int = 42


class STAPPreprocessingLayer:
    """
    Semantic Topology-Aware Projection (STAP) Preprocessing Layer.

    Implements the complete STAP mathematical framework:

    1. High-dimensional connection probabilities:
       P(i,j) = exp(-||x_i - x_j||² / σ_i)

    2. Low-dimensional connection probabilities:
       Q(i,j) = (1 + a||y_i - y_j||^(2b))^(-1)

    3. Cross-entropy objective:
       L = L_attract + L_repel
       L_attract = Σ P(i,j) log(P(i,j) / Q(i,j))
       L_repel = Σ (1 - P(i,k)) log((1 - P(i,k)) / (1 - Q(i,k)))

    4. Optimization via stochastic gradient descent with negative sampling
    """

    def __init__(self, config: Optional[STAPConfig] = None):
        """
        Initialize STAP preprocessing layer.

        Args:
            config: STAP configuration object
        """
        self.config = config or STAPConfig()

        # Initialize sentence transformer for high-dimensional embeddings
        self.embedding_model = SentenceTransformer(self.config.embedding_model_name)
        self.embedding_dim = self.embedding_model.get_sentence_embedding_dimension()

        # Storage for global state
        self.high_dim_embeddings = []  # High-dimensional embeddings (x_i)
        self.low_dim_embeddings = []   # Low-dimensional embeddings (y_i)
        self.user_id_map = {}          # Map user_id to index
        self.index_to_user_id = {}     # Reverse map

        # HNSW index for efficient nearest neighbor search
        self.hnsw_index = None
        self.is_fitted = False

        # Cache for connection probabilities
        self.P_high = None  # High-dimensional connection probabilities
        self.local_sigmas = None  # Locally adaptive scaling parameters

        logger.info(f"STAP v3.0 initialized: {self.config.embedding_model_name} -> {self.config.target_dim}D")

    def generate_semantic_coordinate(
        self,
        user_id: str,
        user_corpus: List[str],
        engagement_patterns: Dict
    ) -> Tuple[np.ndarray, float]:
        """
        Generate semantic coordinate for a user using full STAP framework.

        Process:
        1. Generate high-dimensional text embeddings
        2. Add to global embedding collection
        3. Compute/update high-dimensional connection probabilities
        4. Optimize low-dimensional embedding via gradient descent
        5. Integrate engagement patterns
        6. Calculate confidence score

        Args:
            user_id: Unique user identifier
            user_corpus: List of user's text content
            engagement_patterns: Dict with interaction data

        Returns:
            Tuple of (semantic_coordinate, confidence_score)
        """
        # Step 1: Generate high-dimensional text embeddings
        text_embedding = self._generate_text_embeddings(user_corpus)

        # Step 2: Add to global collection
        if user_id in self.user_id_map:
            # Update existing user
            idx = self.user_id_map[user_id]
            self.high_dim_embeddings[idx] = text_embedding
        else:
            # New user
            idx = len(self.high_dim_embeddings)
            self.high_dim_embeddings.append(text_embedding)
            self.user_id_map[user_id] = idx
            self.index_to_user_id[idx] = user_id

            # Initialize low-dimensional embedding randomly
            initial_embedding = np.random.randn(self.config.target_dim) * 0.01
            self.low_dim_embeddings.append(initial_embedding)

        # Step 3: Recompute STAP projection if enough users
        if len(self.high_dim_embeddings) >= self.config.n_neighbors:
            if not self.is_fitted or len(self.high_dim_embeddings) % 5 == 0:
                self._fit_stap_projection()

        # Step 4: Get optimized low-dimensional coordinate
        base_coordinate = self.low_dim_embeddings[idx]

        # Step 5: Integrate engagement patterns
        engagement_vector = self._generate_engagement_vector(
            user_id,
            engagement_patterns
        )

        # Combine text-based coordinate with engagement
        semantic_coordinate = (
            self.config.text_weight * base_coordinate +
            self.config.engagement_weight * engagement_vector
        )

        # Step 6: Normalize to unit sphere
        semantic_coordinate = self._normalize_coordinate(semantic_coordinate)

        # Step 7: Calculate confidence
        confidence = self._calculate_confidence(
            user_corpus,
            engagement_patterns
        )

        # Update HNSW index if fitted
        if self.is_fitted:
            self._update_hnsw_index()

        return semantic_coordinate, confidence

    def update_semantic_coordinate(
        self,
        user_id: str,
        current_coordinate: np.ndarray,
        new_content: List[str],
        new_interactions: Dict,
        learning_rate: float = 0.3
    ) -> Tuple[np.ndarray, float]:
        """
        Incrementally update semantic coordinate with new data.

        Uses exponential moving average for smooth updates.

        Args:
            user_id: User identifier
            current_coordinate: Existing semantic coordinate
            new_content: New text content
            new_interactions: New engagement data
            learning_rate: Update rate (0-1)

        Returns:
            Updated (coordinate, confidence)
        """
        # Generate coordinate for new content
        new_coordinate, new_confidence = self.generate_semantic_coordinate(
            user_id,
            new_content,
            new_interactions
        )

        # Exponential moving average
        updated_coordinate = (
            (1 - learning_rate) * current_coordinate +
            learning_rate * new_coordinate
        )

        # Re-normalize
        updated_coordinate = self._normalize_coordinate(updated_coordinate)

        # Confidence increases with more data
        updated_confidence = min(1.0, new_confidence + 0.05)

        return updated_coordinate, updated_confidence

    def _fit_stap_projection(self):
        """
        Fit STAP projection using complete mathematical framework.

        This is the core STAP algorithm:
        1. Compute high-dimensional connection probabilities P(i,j)
        2. Calculate locally adaptive scaling parameters σ_i
        3. Initialize low-dimensional embeddings
        4. Optimize via stochastic gradient descent
        5. Build HNSW index for efficient queries
        """
        n_samples = len(self.high_dim_embeddings)

        if n_samples < self.config.n_neighbors:
            logger.warning(f"Not enough samples ({n_samples}) for STAP fitting")
            return

        logger.info(f"Fitting STAP projection with {n_samples} samples...")

        # Convert to numpy array
        X = np.array(self.high_dim_embeddings)

        # Step 1: Compute high-dimensional connection probabilities
        self.P_high, self.local_sigmas = self._compute_high_dim_probabilities(X)

        # Step 2: Initialize low-dimensional embeddings if needed
        if len(self.low_dim_embeddings) < n_samples:
            # Initialize remaining embeddings
            for i in range(len(self.low_dim_embeddings), n_samples):
                self.low_dim_embeddings.append(
                    np.random.randn(self.config.target_dim) * 0.01
                )

        Y = np.array(self.low_dim_embeddings)

        # Step 3: Optimize low-dimensional embeddings via SGD
        Y_optimized = self._optimize_stap_objective(X, Y, self.P_high)

        # Step 4: Update low-dimensional embeddings
        self.low_dim_embeddings = [Y_optimized[i] for i in range(n_samples)]

        # Step 5: Build HNSW index for efficient nearest neighbor search
        self._build_hnsw_index(Y_optimized)

        self.is_fitted = True
        logger.info("STAP projection fitted successfully")

    def _compute_high_dim_probabilities(
        self,
        X: np.ndarray
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        Compute high-dimensional connection probabilities P(i,j).

        P(i,j) = exp(-||x_i - x_j||² / σ_i)

        where σ_i is locally adaptive scaling parameter.

        Args:
            X: High-dimensional embeddings (n_samples, embedding_dim)

        Returns:
            Tuple of (P_high, local_sigmas)
        """
        n_samples = X.shape[0]

        # Step 1: Compute pairwise distances
        # Using sklearn for efficiency
        nn = NearestNeighbors(
            n_neighbors=min(self.config.n_neighbors + 1, n_samples),
            metric='euclidean'
        )
        nn.fit(X)
        distances, indices = nn.kneighbors(X)

        # Step 2: Calculate locally adaptive σ_i for each point
        # σ_i = distance to kth nearest neighbor
        # This accounts for varying neighborhood densities
        local_sigmas = distances[:, -1]  # Distance to farthest neighbor

        # Avoid division by zero
        local_sigmas = np.maximum(local_sigmas, 1e-10)

        # Step 3: Compute connection probabilities
        P_high = np.zeros((n_samples, n_samples))

        for i in range(n_samples):
            for j_idx, j in enumerate(indices[i]):
                if i != j:
                    # P(i,j) = exp(-d²/σ_i)
                    d_squared = distances[i, j_idx] ** 2
                    P_high[i, j] = np.exp(-d_squared / local_sigmas[i])

        # Symmetrize probabilities
        P_high = (P_high + P_high.T) / 2

        # Normalize rows to sum to 1
        row_sums = P_high.sum(axis=1, keepdims=True)
        row_sums = np.maximum(row_sums, 1e-10)
        P_high = P_high / row_sums

        logger.info(f"Computed high-dimensional probabilities: σ range [{local_sigmas.min():.4f}, {local_sigmas.max():.4f}]")

        return P_high, local_sigmas

    def _optimize_stap_objective(
        self,
        X: np.ndarray,
        Y_init: np.ndarray,
        P_high: np.ndarray
    ) -> np.ndarray:
        """
        Optimize STAP objective function via stochastic gradient descent.

        Objective:
        L = L_attract + L_repel

        L_attract = Σ P(i,j) log(P(i,j) / Q(i,j))
        L_repel = Σ (1 - P(i,k)) log((1 - P(i,k)) / (1 - Q(i,k)))

        where Q(i,j) = (1 + a||y_i - y_j||^(2b))^(-1)

        Args:
            X: High-dimensional embeddings
            Y_init: Initial low-dimensional embeddings
            P_high: High-dimensional connection probabilities

        Returns:
            Optimized low-dimensional embeddings
        """
        n_samples = X.shape[0]
        Y = Y_init.copy()

        # Learning rate schedule
        initial_lr = self.config.learning_rate
        min_lr = 0.01

        # Optimization parameters
        a = self.config.a
        b = self.config.b
        n_neg_samples = self.config.negative_sample_rate

        logger.info(f"Starting STAP optimization: {self.config.n_epochs} epochs")

        for epoch in range(self.config.n_epochs):
            # Adaptive learning rate
            lr = initial_lr * (1 - epoch / self.config.n_epochs) + min_lr

            # Shuffle sample order
            indices = np.random.permutation(n_samples)

            epoch_loss = 0.0

            for i in indices:
                # Get neighbors of i (where P(i,j) > 0)
                neighbors = np.where(P_high[i] > 1e-10)[0]

                if len(neighbors) == 0:
                    continue

                # Attractive forces (preserve local structure)
                grad_attract = np.zeros(self.config.target_dim)

                for j in neighbors:
                    if i == j:
                        continue

                    # Compute Q(i,j)
                    diff = Y[i] - Y[j]
                    dist_squared = np.sum(diff ** 2)

                    # Q(i,j) = (1 + a * dist^(2b))^(-1)
                    Q_ij = 1.0 / (1.0 + a * (dist_squared ** b))

                    # Gradient of attractive component
                    # ∂L_attract/∂y_i = -2ab * P(i,j) * (1 - Q(i,j)) * dist^(2b-2) * (y_i - y_j)
                    grad_coeff = 2 * a * b * P_high[i, j] * (1 - Q_ij)
                    if dist_squared > 0:
                        grad_coeff *= (dist_squared ** (b - 1))

                    grad_attract += grad_coeff * diff

                    # Accumulate loss
                    if Q_ij > 1e-10:
                        epoch_loss += P_high[i, j] * np.log(P_high[i, j] / Q_ij)

                # Repulsive forces (push dissimilar points apart)
                grad_repel = np.zeros(self.config.target_dim)

                # Negative sampling for computational efficiency
                neg_samples = np.random.choice(
                    n_samples,
                    size=min(n_neg_samples, n_samples - 1),
                    replace=False
                )

                for k in neg_samples:
                    if k == i or k in neighbors:
                        continue

                    # Compute Q(i,k)
                    diff = Y[i] - Y[k]
                    dist_squared = np.sum(diff ** 2)

                    Q_ik = 1.0 / (1.0 + a * (dist_squared ** b))

                    # Gradient of repulsive component
                    # ∂L_repel/∂y_i = 2ab * (1 - P(i,k)) * Q(i,k) * dist^(2b-2) * (y_i - y_k)
                    P_ik = P_high[i, k] if P_high[i, k] > 1e-10 else 0.0
                    grad_coeff = -2 * a * b * (1 - P_ik) * Q_ik
                    if dist_squared > 0:
                        grad_coeff *= (dist_squared ** (b - 1))

                    grad_repel += grad_coeff * diff

                    # Accumulate loss
                    if Q_ik < 1.0 - 1e-10:
                        epoch_loss += (1 - P_ik) * np.log((1 - P_ik) / (1 - Q_ik))

                # Total gradient
                grad_total = grad_attract + grad_repel

                # Gradient descent update
                Y[i] -= lr * grad_total

            # Log progress
            if (epoch + 1) % 50 == 0:
                avg_loss = epoch_loss / n_samples
                logger.info(f"Epoch {epoch + 1}/{self.config.n_epochs}: Loss = {avg_loss:.6f}, LR = {lr:.4f}")

        logger.info("STAP optimization complete")
        return Y

    def _build_hnsw_index(self, Y: np.ndarray):
        """
        Build HNSW index for O(log n) nearest neighbor queries.

        HNSW (Hierarchical Navigable Small World) provides:
        - Construction: O(n log n)
        - Query: O(log n)

        Args:
            Y: Low-dimensional embeddings
        """
        n_samples = Y.shape[0]

        # Initialize HNSW index
        self.hnsw_index = hnswlib.Index(space='cosine', dim=self.config.target_dim)

        # Parameters for HNSW
        # M: number of connections per layer (16 is good default)
        # ef_construction: size of dynamic candidate list (200 is good default)
        self.hnsw_index.init_index(
            max_elements=n_samples * 2,  # Allow for growth
            ef_construction=200,
            M=16
        )

        # Add embeddings to index
        self.hnsw_index.add_items(Y, np.arange(n_samples))

        # Set ef for query (higher = more accurate but slower)
        self.hnsw_index.set_ef(50)

        logger.info(f"HNSW index built with {n_samples} elements")

    def _update_hnsw_index(self):
        """Update HNSW index with new embeddings"""
        if self.hnsw_index is None:
            return

        current_count = self.hnsw_index.get_current_count()
        n_embeddings = len(self.low_dim_embeddings)

        if n_embeddings > current_count:
            # Add new embeddings
            new_embeddings = np.array(self.low_dim_embeddings[current_count:])
            new_indices = np.arange(current_count, n_embeddings)
            self.hnsw_index.add_items(new_embeddings, new_indices)

    def find_nearest_neighbors(
        self,
        query_coordinate: np.ndarray,
        k: int = 10
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        Find k nearest neighbors using HNSW index.

        Args:
            query_coordinate: Query point in semantic space
            k: Number of neighbors to return

        Returns:
            Tuple of (neighbor_indices, distances)
        """
        if not self.is_fitted or self.hnsw_index is None:
            raise ValueError("STAP must be fitted before querying")

        # Query HNSW index
        labels, distances = self.hnsw_index.knn_query(
            query_coordinate.reshape(1, -1),
            k=k
        )

        return labels[0], distances[0]

    def _generate_text_embeddings(self, corpus: List[str]) -> np.ndarray:
        """
        Generate semantic embeddings from text corpus.

        Args:
            corpus: List of text strings

        Returns:
            Mean embedding vector
        """
        if not corpus:
            return np.zeros(self.embedding_dim)

        # Encode all texts
        embeddings = self.embedding_model.encode(corpus)

        # Average embeddings
        mean_embedding = np.mean(embeddings, axis=0)

        return mean_embedding

    def _generate_engagement_vector(
        self,
        user_id: str,
        engagement_patterns: Dict
    ) -> np.ndarray:
        """
        Generate engagement-based vector from interaction patterns.

        Args:
            user_id: User identifier
            engagement_patterns: Dict with interaction data

        Returns:
            Engagement vector in target dimensional space
        """
        engagement_vector = np.zeros(self.config.target_dim)

        if not engagement_patterns:
            return engagement_vector

        # Extract engagement features
        like_intensity = engagement_patterns.get('like_intensity', 0.0)
        share_intensity = engagement_patterns.get('share_intensity', 0.0)
        comment_intensity = engagement_patterns.get('comment_intensity', 0.0)

        # Interaction diversity
        interaction_types = engagement_patterns.get('interaction_types', [])
        diversity = len(set(interaction_types)) / max(len(interaction_types), 1)

        # Create structured engagement representation
        third = self.config.target_dim // 3

        engagement_vector[:third] = like_intensity
        engagement_vector[third:2*third] = share_intensity
        engagement_vector[2*third:] = comment_intensity * diversity

        return engagement_vector

    def _normalize_coordinate(self, coordinate: np.ndarray) -> np.ndarray:
        """Normalize coordinate to unit sphere"""
        norm = np.linalg.norm(coordinate)
        if norm > 0:
            return coordinate / norm
        return coordinate

    def _calculate_confidence(
        self,
        corpus: List[str],
        engagement_patterns: Dict
    ) -> float:
        """
        Calculate confidence score for semantic coordinate.

        Args:
            corpus: Text corpus
            engagement_patterns: Engagement data

        Returns:
            Confidence score [0, 1]
        """
        # Text data confidence
        text_confidence = min(1.0, len(corpus) / 20.0)

        # Engagement confidence
        total_interactions = sum(engagement_patterns.values()) if engagement_patterns else 0
        engagement_confidence = min(1.0, total_interactions / 50.0)

        # Combined confidence
        confidence = 0.6 * text_confidence + 0.4 * engagement_confidence

        return confidence

    def get_projection_quality_metrics(self) -> Dict:
        """
        Calculate metrics to assess STAP projection quality.

        Returns:
            Dict with quality metrics
        """
        if not self.is_fitted:
            return {"status": "not_fitted"}

        n_samples = len(self.high_dim_embeddings)

        if n_samples < 10:
            return {"status": "insufficient_data"}

        X = np.array(self.high_dim_embeddings)
        Y = np.array(self.low_dim_embeddings)

        # Compute neighbor preservation
        k = min(5, n_samples - 1)

        # High-dimensional neighbors
        nn_high = NearestNeighbors(n_neighbors=k, metric='cosine')
        nn_high.fit(X)
        neighbors_high = nn_high.kneighbors(X, return_distance=False)

        # Low-dimensional neighbors
        nn_low = NearestNeighbors(n_neighbors=k, metric='euclidean')
        nn_low.fit(Y)
        neighbors_low = nn_low.kneighbors(Y, return_distance=False)

        # Calculate overlap
        overlaps = []
        for i in range(n_samples):
            overlap = len(set(neighbors_high[i]) & set(neighbors_low[i])) / k
            overlaps.append(overlap)

        return {
            "status": "fitted",
            "n_users": n_samples,
            "neighbor_preservation": np.mean(overlaps),
            "neighbor_preservation_std": np.std(overlaps),
            "embedding_dim": self.embedding_dim,
            "target_dim": self.config.target_dim,
            "local_sigma_range": [float(self.local_sigmas.min()), float(self.local_sigmas.max())],
            "optimization_epochs": self.config.n_epochs
        }
