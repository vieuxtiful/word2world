"""
Word2World v4.0 - Social Platform Engine (GitHub Version)
Advanced multi-modal embedding system with robust covariance estimation,
stable hyperbolic geometry, and manifold-aware regularization.

Authors: Shu Bai, Xiaoyue Cao, Jinxing Chen, Yunzhou Dai, Vieux Valcin, Huiyi Zhang
Date: October 24, 2025
Version: 4.0
License: MIT

Installation:
    pip install torch transformers sentence-transformers scikit-learn networkx gensim nltk numpy pandas matplotlib seaborn plotly

Environment Variables:
    HF_TOKEN: Hugging Face API token for model access

Usage:
    from word2world import STAPv4PreprocessingLayer, STAPv4Config
    
    config = STAPv4Config()
    stap = STAPv4PreprocessingLayer(config)
    
    coordinate, confidence = stap.generate_semantic_coordinate(
        user_id="user123",
        user_corpus=["Building bridges through dialogue"],
        engagement_patterns={},
        user_images=[],
        user_connections=[]
    )
"""

import userdata
import os

try:
    HF_TOKEN = userdata.get('HF_TOKEN')
    os.environ['HF_TOKEN'] = HF_TOKEN
    print("‚úì HF_TOKEN loaded from Colab Secrets")
except:
    print("‚ö†Ô∏è  HF_TOKEN not found in Colab Secrets")
    print("   Add it via the üîë icon in the left sidebar")

# ============================================================================
# IMPORTS
# ============================================================================

# -*- coding: utf-8 -*-
"""Word2World.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1viyxYSiHaHJCac_U9R3RXLdDtVpkBL5K

**Word2World Social Media Platform** <br>
**Bridging Ideas Together** <br>
Authors: Shu Bai, Xiaoyue Cao, Jinxing Chen, Yunzhou Dai, Vieux Valcin, Huiyi Zhang <br>
Version: 4.0 <br>
Updated: October 22, 2025 <br>
License: MIT
"""

# -*- coding: utf-8 -*-
"""
Word2World Social Platform - STAP v4.0 (Google Colab Version)
Bridging Ideas Together

Multi-Modal Hyperbolic Embeddings for Peace-Building Social Networks

Features:
- Multi-modal user representation (text + image + network + engagement)
- Mahalanobis distance with covariance awareness
- Hyperbolic geometry for hierarchical semantic space
- Siamese neural network for bridge prediction
- Secure Hugging Face token handling

Authors: Shu Bai, Xiaoyue Cao, Jinxing Chen, Yunzhou Dai, Vieux Valcin, Huiyi Zhang
Version: 4.0
Updated: October 22, 2025
License: MIT

Instructions for Google Colab:
1. Upload this file to Google Colab
2. Add your Hugging Face token to Colab Secrets:
   - Click the key icon (üîë) in the left sidebar
   - Add a new secret: Name = "HF_TOKEN", Value = your token
3. Run all cells
"""

"""# Intro

Multimodal Topology-Aware Semantic Projection (STAP)

**What is "STAP?" ‚á©**

Word2World implements an advanced Semantic Topology-Aware Projection (STAP) framework integrating multimodal data for user and content representation. The updated STAP (v4.0) combines:

High-Dimensional Semantic Embeddings:
- Text: Sentence Transformers produce 384-dimensional embeddings capturing nuanced meaning.
- Images: OpenAI CLIP (Contrastive Language‚ÄìImage Pretraining) generates aligned 512-dimensional image-text embeddings, supporting rich cross-modal semantic associations.
- Network: Node2Vec computes 128-dimensional graph embeddings, representing social relational structures and community context.

Manifold Learning via UMAP:
- Uniform Manifold Approximation and Projection preserves topological structure while reducing fused multimodal vectors to a shared low-dimensional (e.g., 32D) space, maintaining critical neighborhood relationships.

Engagement Integration:
- Behavioral signals (likes, shares, comments) are embedded and fused with multimodal semantic and relational representations, yielding a holistic user/content vector.

Adaptive Confidence Scoring:
- Reliability of each coordinate is quantified by the diversity and density of available modalities (semantic, visual, network, engagement), assigning greater confidence to well-supported projections.

**Mathematical Foundation**
The STAP projection still optimizes a cross-entropy objective:
L = L_attract + L_repel

Where:
- L_attract preserves local neighborhood and semantic/structural similarity (including text, vision, and network proximity)
- L_repel promotes separation of dissimilar or potentially contentious content/users

This formulation upholds topological integrity for bridge identification, and is made efficient by:
- UMAP's stochastic gradient descent across fused multimodal space
- Approximate nearest neighbor search (O(log n) complexity)
- Incremental learning strategies as new data and users are added

**Bridge Identification**

Given multimodal semantic coordinates y_iand y_j for users/items, the generalized bridge match strength is:

B(i,j)=f(d(y_i,y_j))√óC(i,j)
Where:

d(y_i,y_j) integrates cosine or other distance metrics over fused text, image (CLIP), network (Node2Vec), and behavioral coordinates.

f(‚ãÖ) is a scoring function calibrated for optimal bridge discovery, empirically in the [0.3, 0.5] range.

C(i,j) is an adaptive confidence weighting, reflecting both modality agreement and data robustness.

This ensures content and connection recommendations forge productive cross-domain connections (‚Äúbridges‚Äù)‚Äîleveraging not only textual meaning, but also visual context and social structure‚Äîwhile minimizing conflict potential.

# NEW SETUP & INSTALLATION
"""

"""Word2World - STAP v4.0 Google Colab Setup Instructions
=========================================================

This notebook implements the STAP v4.0 (Semantic Topology-Aware Projection)
framework for the Word2World social media platform with a "Siamese" NN for "bridge"-aware scoring.

STAP v4.0 Features:
- Multi-Modal Mahalanobis Distance for high-dimensional semantic space
- Hyperbolic Geometry (Poincar√© Ball) for low-dimensional representation
- Contrastive Loss for niche boundary classification
- Siamese Neural Network for bridge-aware recommendation scoring

Authors: Shu Bai, Xiaoyue Cao, Jinxing Chen, Yunzhou Dai, Vieux Valcin, Huiyi Zhang
Version: 4.0
Date: October 19, 2025
License: MIT
"""



# ============================================================================
# ROBUST COVARIANCE ESTIMATION
# ============================================================================

@dataclass
class CovarianceConfig:
    """Configuration for covariance estimation strategies."""

    method: Literal['classical', 'robust', 'cellwise'] = 'robust'
    rank: Optional[int] = None  # Auto: ceil(sqrt(d))
    epsilon: float = 1e-6
    mcd_support_fraction: Optional[float] = None  # Auto-determined by MCD

    def get_rank(self, n_features: int) -> int:
        """Get rank for low-rank approximation."""
        if self.rank is not None:
            return min(self.rank, n_features)
        return max(1, int(np.ceil(np.sqrt(n_features))))


class RobustCovarianceEstimator:
    """
    Robust covariance estimator using MCD with low-rank + diagonal decomposition.

    Implements Section 2.1.2 of the technical paper:
    - Minimum Covariance Determinant for outlier resistance
    - Low-rank approximation via SVD for scalability
    - Diagonal residual for full-rank representation
    - Cholesky decomposition for efficient distance computation
    """

    def __init__(self, config: CovarianceConfig):
        """
        Initialize robust covariance estimator.

        Args:
            config: Covariance estimation configuration
        """
        self.config = config
        self.cov = None
        self.L = None
        self.mu = None
        self.rank = None
        self.U_k = None
        self.D = None
        self.mcd_estimator = None
        self.is_fitted = False

    def fit(self, X: np.ndarray) -> 'RobustCovarianceEstimator':
        """
        Fit robust covariance estimator.

        Args:
            X: (N, d) data matrix

        Returns:
            self
        """
        N, d = X.shape

        if N < 2:
            logger.warning("Insufficient samples for covariance estimation")
            return self

        self.rank = self.config.get_rank(d)

        if self.config.method == 'classical':
            self._fit_classical(X)
        elif self.config.method == 'robust':
            self._fit_robust(X)
        elif self.config.method == 'cellwise':
            self._fit_cellwise(X)
        else:
            raise ValueError(f"Unknown method: {self.config.method}")

        self.is_fitted = True
        return self

    
    def fit_shrinkage(self, X: np.ndarray, method: str = 'ledoit_wolf', target: str = 'diagonal'):
        """
        Fit covariance using shrinkage estimation.
        
        Œ£_shrink = Œ±¬∑S + (1-Œ±)¬∑T
        where S = sample covariance, T = target, Œ± = shrinkage intensity
        
        Args:
            X: Data matrix (N, d)
            method: 'ledoit_wolf' or 'oas'
            target: 'diagonal', 'identity', or 'pooled'
        """
        from sklearn.covariance import LedoitWolf, OAS
        
        if method == 'ledoit_wolf':
            estimator = LedoitWolf()
        elif method == 'oas':
            estimator = OAS()
        else:
            raise ValueError(f"Unknown shrinkage method: {method}")
        
        estimator.fit(X)
        self.cov_ = estimator.covariance_
        self.mu_ = estimator.location_
        self.shrinkage_alpha_ = getattr(estimator, 'shrinkage_', None)
        
        # Apply low-rank decomposition if configured
        if self.config.covariance_rank is not None:
            self._apply_low_rank_decomposition()
        
        self._compute_cholesky()
        
        return self
    
    def compute_local_covariances(self, X: np.ndarray, k: int = 10, kernel: str = 'gaussian'):
        """
        Compute local covariance matrices in k-nearest neighborhoods.
        
        Args:
            X: Data matrix (N, d)
            k: Number of nearest neighbors
            kernel: 'gaussian', 'uniform', or 'none'
        
        Returns:
            List of local covariance matrices
        """
        from sklearn.neighbors import NearestNeighbors
        
        nbrs = NearestNeighbors(n_neighbors=k, algorithm='auto').fit(X)
        local_covs = []
        local_mus = []
        
        for i in range(X.shape[0]):
            _, idxs = nbrs.kneighbors([X[i]])
            neighbors = X[idxs[0]]
            
            # Compute local mean
            mu_local = np.mean(neighbors, axis=0)
            
            # Compute local covariance with optional kernel weighting
            if kernel == 'gaussian':
                distances = np.linalg.norm(neighbors - X[i], axis=1)
                sigma = np.median(distances) + 1e-10
                weights = np.exp(-distances**2 / (2 * sigma**2))
                weights = weights / np.sum(weights)
                
                # Weighted covariance
                centered = neighbors - mu_local
                cov_local = (centered.T @ np.diag(weights) @ centered)
            elif kernel == 'uniform':
                cov_local = np.cov(neighbors.T)
            else:  # 'none'
                cov_local = np.cov(neighbors.T)
            
            # Regularize
            cov_local += np.eye(cov_local.shape[0]) * self.config.covariance_epsilon
            
            local_covs.append(cov_local)
            local_mus.append(mu_local)
        
        return local_covs, local_mus
    
    def fit_with_local_refinement(self, X: np.ndarray, k: int = 10):
        """
        Fit global covariance then refine with local estimates.
        
        Args:
            X: Data matrix (N, d)
            k: Number of nearest neighbors for local estimation
        """
        # First fit global covariance
        self.fit(X)
        
        # Compute local covariances
        local_covs, local_mus = self.compute_local_covariances(X, k=k)
        
        # Store for potential use in distance computation
        self.local_covs_ = local_covs
        self.local_mus_ = local_mus
        self.has_local_covs_ = True
        
        return self

    def _fit_classical(self, X: np.ndarray):
        """Classical sample covariance with low-rank decomposition."""
        self.mu = np.mean(X, axis=0)
        X_centered = X - self.mu

        # Sample covariance
        cov_full = (X_centered.T @ X_centered) / (X.shape[0] - 1)

        # Low-rank decomposition
        self._decompose_covariance(cov_full)

    def _fit_robust(self, X: np.ndarray):
        """Robust covariance using MCD with low-rank decomposition."""
        # MCD estimation
        mcd = MinCovDet(
            support_fraction=self.config.mcd_support_fraction,
            random_state=42
        )

        try:
            mcd.fit(X)
            self.mcd_estimator = mcd
            self.mu = mcd.location_
            cov_robust = mcd.covariance_

            # Low-rank decomposition
            self._decompose_covariance(cov_robust)

            logger.info(f"MCD fitted: support={mcd.support_.sum()}/{X.shape[0]}")

        except Exception as e:
            logger.warning(f"MCD failed: {e}, falling back to classical")
            self._fit_classical(X)

    def _fit_cellwise(self, X: np.ndarray):
        """Cellwise robust covariance (placeholder for RobPy integration)."""
        logger.warning("Cellwise MCD not implemented, using robust MCD")
        self._fit_robust(X)

    def _decompose_covariance(self, cov_full: np.ndarray):
        """
        Decompose covariance into low-rank + diagonal.

        Œ£ ‚âà U_k U_k^T + D

        Args:
            cov_full: Full covariance matrix
        """
        d = cov_full.shape[0]

        # SVD decomposition
        U, S, Vt = np.linalg.svd(cov_full)

        # Low-rank factor: U_k * sqrt(S_k)
        self.U_k = U[:, :self.rank] * np.sqrt(S[:self.rank])

        # Diagonal residual
        residual = cov_full - self.U_k @ self.U_k.T
        self.D = np.diag(np.diag(residual))

        # Reconstruct approximation
        self.cov = self.U_k @ self.U_k.T + self.D

        # Add regularization
        self.cov += self.config.epsilon * np.eye(d)

        # Cholesky decomposition
        try:
            self.L = cholesky(self.cov, lower=True)
        except np.linalg.LinAlgError:
            logger.warning("Cholesky failed, increasing regularization")
            self.cov += 10 * self.config.epsilon * np.eye(d)
            self.L = cholesky(self.cov, lower=True)

        logger.info(f"Covariance decomposed: rank={self.rank}, dim={d}")

    def mahalanobis_distance(self, x: np.ndarray, y: np.ndarray) -> float:
        """
        Compute Mahalanobis distance using Cholesky factor.

        d_M(x, y) = sqrt((x-y)^T Œ£^{-1} (x-y))
                  = ||L^{-1}(x-y)||_2

        Args:
            x: First vector
            y: Second vector

        Returns:
            Mahalanobis distance
        """
        if not self.is_fitted:
            return np.linalg.norm(x - y)

        diff = np.asarray(x) - np.asarray(y)
        z = solve_triangular(self.L, diff, lower=True)
        return np.sqrt(np.dot(z, z))

    def transform(self, X: np.ndarray) -> np.ndarray:
        """
        Transform data to whitened space: Z = L^{-1}(X - Œº).

        Args:
            X: (N, d) data matrix

        Returns:
            (N, d) whitened data
        """
        if not self.is_fitted:
            return X

        X_centered = X - self.mu
        return solve_triangular(self.L, X_centered.T, lower=True).T

    def get_covariance(self) -> np.ndarray:
        """Get estimated covariance matrix."""
        return self.cov if self.is_fitted else None

    def get_precision(self) -> np.ndarray:
        """Get precision matrix (inverse covariance)."""
        if not self.is_fitted:
            return None
        return np.linalg.inv(self.cov)


# ============================================================================
# SYSTEM REQUIREMENTS & INSTALLATION
# ============================================================================

print("=" * 80)
print("Word2World - Setup")
print("=" * 80)

# Check Python version
import sys
print(f"\n‚úì Python Version: {sys.version}")
assert sys.version_info >= (3, 8), "Python 3.8+ required"

# Install required packages
print("\nüì¶ Installing dependencies...")
print("-" * 80)

# Core dependencies
!pip install -q numpy==1.24.0
!pip install -q scipy==1.10.0
!pip install -q scikit-learn==1.3.0

# Deep learning
!pip install -q torch==2.0.0
!pip install -q torchvision==0.15.0

# NLP and embeddings
!pip install -q sentence-transformers==2.2.2
!pip install -q transformers==4.30.0

# STAP-specific
!pip install -q umap-learn==0.5.4
!pip install -q pynndescent==0.5.10

# Fast nearest neighbors
!pip install -q hnswlib==0.7.0

# Utilities
!pip install -q tqdm==4.65.0

print("\n‚úì All dependencies installed successfully!")

# ============================================================================
# SYSTEM SPECIFICATIONS
# ============================================================================

print("\n" + "=" * 80)
print("SYSTEM SPECIFICATIONS")
print("=" * 80)

specs = """
Core Dependencies:
------------------
numpy==1.24.0              # Numerical computing
scipy==1.10.0              # Scientific computing (Cholesky, distance metrics)
scikit-learn==1.3.0        # Machine learning utilities

Deep Learning:
--------------
torch==2.0.0               # PyTorch for Siamese network
torchvision==0.15.0        # Vision utilities

NLP & Embeddings:
-----------------
sentence-transformers==2.2.2   # Semantic text embeddings
transformers==4.30.0           # Hugging Face transformers

STAP-Specific:
--------------
umap-learn==0.5.4          # Manifold learning (fallback)
pynndescent==0.5.10        # Fast nearest neighbors (UMAP dependency)
hnswlib==0.7.0             # Hierarchical Navigable Small World graphs

Utilities:
----------
tqdm==4.65.0               # Progress bars

Hardware Requirements:
----------------------
Minimum:
  - CPU: 2 cores
  - RAM: 8 GB
  - Storage: 2 GB

Recommended:
  - CPU: 4+ cores
  - RAM: 16 GB
  - GPU: NVIDIA with CUDA support (for Siamese network training)
  - Storage: 10 GB

Optimal (Production):
  - CPU: 8+ cores
  - RAM: 32 GB
  - GPU: NVIDIA V100/A100 or equivalent
  - Storage: 50 GB SSD

Colab Environment:
------------------
  - Standard: 12 GB RAM, 2 CPU cores (sufficient for testing)
  - Pro: 25 GB RAM, GPU access (recommended for training)
  - Pro+: 50 GB RAM, premium GPU (optimal for production)

Performance Benchmarks:
-----------------------
Operation                    | Time (N=1000) | Time (N=10000)
-----------------------------|---------------|----------------
Text Embedding Generation    | 2.5 sec       | 25 sec
Mahalanobis Distance (Diag)  | 0.01 sec      | 0.1 sec
Mahalanobis Distance (Full)  | 0.05 sec      | 0.5 sec
Hyperbolic Distance          | 0.001 sec     | 0.01 sec
STAP Optimization (100 ep)   | 15 sec        | 180 sec
HNSW Index Build             | 0.5 sec       | 8 sec
HNSW Query (k=10)            | 0.0001 sec    | 0.0001 sec
Siamese Inference (batch)    | 0.01 sec      | 0.05 sec

Memory Footprint:
-----------------
Component                    | Memory (N=1000) | Memory (N=10000)
-----------------------------|-----------------|------------------
Text Embeddings (384D)       | 3 MB            | 30 MB
Multi-modal Embeddings       | 8 MB            | 80 MB
Covariance Matrix (Diag)     | 8 KB            | 80 KB
Covariance Matrix (Full)     | 8 MB            | 800 MB
Low-dim Embeddings (32D)     | 0.25 MB         | 2.5 MB
HNSW Index                   | 2 MB            | 20 MB
Siamese Network              | 5 MB            | 5 MB
Total (Diagonal Cov)         | ~20 MB          | ~140 MB
Total (Full Cov)             | ~25 MB          | ~940 MB

Scalability:
------------
Users (N)    | Embedding Time | STAP Time | HNSW Query | Total RAM
-------------|----------------|-----------|------------|------------
1,000        | 25 sec         | 15 sec    | 0.0001 sec | 20 MB
10,000       | 250 sec        | 180 sec   | 0.0001 sec | 140 MB
100,000      | 2,500 sec      | 1,800 sec | 0.0001 sec | 1.4 GB
1,000,000    | 25,000 sec     | 18,000 sec| 0.0001 sec | 14 GB

Note: STAP optimization can be parallelized and run incrementally
"""

print(specs)

# ============================================================================
# VERIFY INSTALLATION
# ============================================================================

print("\n" + "=" * 80)
print("VERIFYING INSTALLATION")
print("=" * 80)

try:
    import numpy as np
    print("‚úì NumPy imported successfully")

    import scipy
    print("‚úì SciPy imported successfully")

    from sklearn.neighbors import NearestNeighbors
    print("‚úì scikit-learn imported successfully")

    import torch
    print(f"‚úì PyTorch imported successfully (version: {torch.__version__})")
    print(f"  CUDA available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"  CUDA device: {torch.cuda.get_device_name(0)}")

    from sentence_transformers import SentenceTransformer
    print("‚úì Sentence Transformers imported successfully")

    import hnswlib
    print("‚úì HNSW imported successfully")

    print("\n‚úÖ All packages verified!")

except ImportError as e:
    print(f"\n‚ùå Import error: {e}")
    print("Please run the installation cell again.")

# ============================================================================
# QUICK START EXAMPLE
# ============================================================================

print("\n" + "=" * 80)
print("QUICK START EXAMPLE")
print("=" * 80)

example_code = '''
# After uploading word2world_social_v4.py to Colab, run:

from word2world_social_v4 import STAPv4Config, STAPv4PreprocessingLayer

# Configure STAP v4.0
config = STAPv4Config(
    use_mahalanobis=True,        # Enable multi-modal Mahalanobis distance
    use_hyperbolic=True,          # Enable hyperbolic geometry
    use_contrastive_loss=True,    # Enable contrastive loss
    use_siamese=False,            # Disable Siamese (requires training data)
    n_epochs=100,                 # Optimization epochs
    target_dim=32                 # Low-dimensional space dimension
)

# Initialize STAP v4.0
stap = STAPv4PreprocessingLayer(config)

# Generate semantic coordinate for a user
user_corpus = [
    "I believe renewable energy is the future.",
    "Solar and wind power are becoming more affordable.",
    "We need to transition away from fossil fuels."
]

engagement_patterns = {
    "likes": 15,
    "shares": 8,
    "comments": 5
}

coordinate, confidence = stap.generate_semantic_coordinate(
    user_id="user_001",
    user_corpus=user_corpus,
    engagement_patterns=engagement_patterns
)

print(f"Semantic Coordinate: {coordinate[:5]}... (shape: {coordinate.shape})")
print(f"Confidence Score: {confidence:.3f}")

# Find similar users
neighbors, distances = stap.find_nearest_neighbors(coordinate, k=5)
print(f"Nearest Neighbors: {neighbors}")
print(f"Distances: {distances}")
'''

print(example_code)

print("\n" + "=" * 80)
print("Setup complete! Ready to use STAP v4.0 üöÄ")
print("=" * 80)

# ============================================================================
# CONFIGURATION TEMPLATES
# ============================================================================

print("\n" + "=" * 80)
print("CONFIGURATION TEMPLATES")
print("=" * 80)

templates = """
1. Conservative (Testing):
--------------------------
config = STAPv4Config(
    use_mahalanobis=True,
    use_diagonal_covariance=True,  # Efficient diagonal approximation
    use_hyperbolic=False,           # Use Euclidean space
    use_contrastive_loss=False,
    use_siamese=False,
    n_epochs=50
)

2. Standard (Production):
-------------------------
config = STAPv4Config(
    use_mahalanobis=True,
    use_diagonal_covariance=True,
    use_hyperbolic=True,            # Enable hyperbolic geometry
    use_contrastive_loss=True,      # Enable contrastive loss
    use_siamese=False,
    n_epochs=100
)

3. Advanced (With Siamese):
---------------------------
config = STAPv4Config(
    use_mahalanobis=True,
    use_diagonal_covariance=False,  # Full covariance matrix
    use_hyperbolic=True,
    use_contrastive_loss=True,
    use_siamese=True,               # Enable Siamese network
    siamese_hidden_dim=128,
    n_epochs=200
)

4. High-Performance (Large Scale):
-----------------------------------
config = STAPv4Config(
    use_mahalanobis=True,
    use_diagonal_covariance=True,   # Diagonal for efficiency
    use_hyperbolic=True,
    use_contrastive_loss=True,
    use_siamese=True,
    n_epochs=100,
    learning_rate=0.01,
    negative_sample_rate=3          # Reduce for speed
)
"""

print(templates)

print("\n‚ú®Ready to bridge!‚ú®\n")

"""# Code"""

"""
Original file is located at
    https://colab.research.google.com/drive/1viyxYSiHaHJCac_U9R3RXLdDtVpkBL5K
"""

# Install required packages (Colab-specific)Word2World.ipynb
!pip install -q hnswlib==0.7.0
!pip install -q node2vec

import os
import sys
import logging
from typing import List, Dict, Tuple, Optional, Union, Any
from dataclasses import dataclass
import numpy as np
from scipy.linalg import cholesky, solve_triangular
# Robust covariance estimation
from robust_covariance import RobustCovarianceEstimator, CovarianceConfig

from sklearn.neighbors import NearestNeighbors
import hnswlib

# Deep learning
import torch
import torch.nn as nn
import torch.nn.functional as F

# NLP and embeddings
from sentence_transformers import SentenceTransformer

# CLIP for image embeddings
from transformers import CLIPModel, CLIPProcessor
from PIL import Image
import requests
from io import BytesIO

# Node2Vec for network embeddings
import networkx as nx
try:
    from node2vec import Node2Vec as Node2VecModel
except ImportError:
    Node2VecModel = None

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ============================================================================
# Secure Token Handling
# ============================================================================

def get_hf_token() -> Optional[str]:
    """
    Safely retrieve Hugging Face token from environment variables or Colab secrets.
    This function is secure for GitHub as it never hardcodes tokens.

    Returns:
        HF token string or None if not available
    """
    # Try environment variable first (for local development)
    token = os.getenv('HF_TOKEN')
    if token:
        logger.info("HF token loaded from environment variable")
        return token

    # Try Colab secrets (for Google Colab)
    try:
        from google.colab import userdata
        token = userdata.get('HF_TOKEN')
        logger.info("HF token loaded from Colab secrets")
        return token
    except ImportError:
        logger.debug("Not running in Colab environment")
    except userdata.SecretNotFoundError:
        logger.warning("HF_TOKEN secret not found in Colab")
    except Exception as e:
        logger.warning(f"Error accessing Colab secrets: {e}")

    logger.info("No HF token available - using public models only")
    return None

# Get token safely
HF_TOKEN = get_hf_token()

# ============================================================================
# Configuration
# ============================================================================

@dataclass
class STAPv4Config:
    """Configuration for STAP v4.0 processor with multi-modal support"""

    # Embedding models
    text_embedding_model: str = 'all-MiniLM-L6-v2'
    clip_model: str = 'openai/clip-vit-base-patch32'

    # Multi-modal fusion weights
    alpha_text: float = 0.54
    alpha_image: float = 0.18
    alpha_network: float = 0.18
    alpha_engagement: float = 0.10

    # High-dimensional space
    use_mahalanobis: bool = True
    use_diagonal_covariance: bool = True
    covariance_epsilon: float = 1e-6
    # Covariance estimation strategy
    covariance_method: str = 'robust'  # 'classical', 'robust', 'cellwise'
    covariance_rank: Optional[int] = None  # Auto: ceil(sqrt(d))
    mcd_support_fraction: Optional[float] = None  # Auto-determined by MCD

    # Low-dimensional space
    target_dim: int = 32
    use_hyperbolic: bool = True
    use_contrastive_loss: bool = True

    # Hyperbolic parameters
    hyperbolic_margin: float = 2.0
    contrastive_weight: float = 0.1
    boundary_regularization: float = 0.01
    boundary_threshold: float = 0.95

    # STAP optimization
    n_neighbors: int = 15
    learning_rate: float = 0.01
    n_epochs: int = 200
    negative_sample_rate: int = 5

    # Heavy-tailed distribution (for Euclidean fallback)
    a: float = 1.577
    b: float = 0.895

    # Siamese network
    use_siamese: bool = True
    siamese_hidden_dim: int = 128
    siamese_output_dim: int = 32
    context_dim: int = 20

    # CLIP parameters
    use_clip: bool = True
    clip_image_size: int = 224

    # Node2Vec parameters
    use_node2vec: bool = True
    node2vec_dimensions: int = 128
    node2vec_walk_length: int = 30
    node2vec_num_walks: int = 200
    node2vec_p: float = 1.0
    node2vec_q: float = 1.0
    node2vec_window_size: int = 10
    node2vec_workers: int = 4

    # General
    random_state: int = 42
    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    # Robust Covariance Estimation
    use_shrinkage: bool = False
    shrinkage_method: str = 'ledoit_wolf'  # 'ledoit_wolf', 'oas'
    shrinkage_target: str = 'diagonal'  # 'diagonal', 'identity', 'pooled'
    use_local_covariance: bool = False
    local_k_neighbors: int = 10
    local_kernel: str = 'gaussian'  # 'gaussian', 'uniform', 'none'
    
    # Hyperbolic Stability
    use_taylor_approximation: bool = True
    taylor_threshold: float = 0.1
    
    # Manifold Regularizers
    use_manifold_regularizers: bool = True
    lambda_manifold: float = 0.3
    lambda_taylor_consistency: float = 0.3
    lambda_curvature: float = 0.25
    lambda_conformality: float = 0.2
    lambda_radius: float = 0.15
    lambda_entropy: float = 0.1
    
    # Momentum Contrastive Learning
    use_moco: bool = False
    moco_queue_size: int = 4096
    moco_momentum: float = 0.999
    
    # Triplet Loss
    use_triplet_loss: bool = False
    triplet_margin: float = 1.0


# ============================================================================
# CLIP Encoder (Secure Version)
# ============================================================================

class CLIPEncoder:
    """
    CLIP encoder for image-text embeddings with secure token handling.
    """

    def __init__(self, model_name: str = "openai/clip-vit-base-patch32",
                 device: str = None, hf_token: Optional[str] = None):
        """
        Initialize CLIP encoder securely.

        Args:
            model_name: CLIP model identifier
            device: Device for computation
            hf_token: Optional Hugging Face token (uses secure retrieval if None)
        """
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')

        # Use provided token or safely retrieve one
        token_to_use = hf_token or HF_TOKEN

        # Load model with authentication if token available
        try:
            if token_to_use:
                self.model = CLIPModel.from_pretrained(
                    model_name,
                    use_auth_token=token_to_use
                )
                self.processor = CLIPProcessor.from_pretrained(
                    model_name,
                    use_auth_token=token_to_use
                )
                logger.info(f"CLIP encoder initialized with authentication: {model_name}")
            else:
                self.model = CLIPModel.from_pretrained(model_name)
                self.processor = CLIPProcessor.from_pretrained(model_name)
                logger.info(f"CLIP encoder initialized without authentication: {model_name}")

        except Exception as e:
            logger.warning(f"Failed to initialize CLIP encoder: {e}")
            # Fallback to without authentication
            try:
                self.model = CLIPModel.from_pretrained(model_name)
                self.processor = CLIPProcessor.from_pretrained(model_name)
                logger.info(f"CLIP encoder fallback initialized: {model_name}")
            except Exception as fallback_error:
                logger.error(f"CLIP encoder completely failed: {fallback_error}")
                raise fallback_error

        self.embedding_dim = 512  # CLIP ViT-B/32
        self.model.to(self.device)
        self.model.eval()

    def encode_images(self, images: List[Union[str, Image.Image, np.ndarray]]) -> np.ndarray:
        """
        Encode images to embeddings.

        Args:
            images: List of images (URLs, PIL Images, or numpy arrays)

        Returns:
            Image embeddings array
        """
        if not images:
            return np.zeros((0, self.embedding_dim))

        processed_images = []
        for img in images:
            if isinstance(img, str):
                if img.startswith(('http://', 'https://')):
                    try:
                        response = requests.get(img, timeout=5)
                        img = Image.open(BytesIO(response.content))
                    except Exception as e:
                        logger.warning(f"Failed to load image from URL: {e}")
                        continue
                else:
                    try:
                        img = Image.open(img)
                    except Exception as e:
                        logger.warning(f"Failed to load image from path: {e}")
                        continue
            elif isinstance(img, np.ndarray):
                img = Image.fromarray(img)
            processed_images.append(img)

        if not processed_images:
            return np.zeros((0, self.embedding_dim))

        with torch.no_grad():
            inputs = self.processor(images=processed_images, return_tensors="pt", padding=True)
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            outputs = self.model.get_image_features(**inputs)
            embeddings = outputs / outputs.norm(dim=-1, keepdim=True)

        return embeddings.cpu().numpy()

    def encode_texts(self, texts: List[str]) -> np.ndarray:
        """
        Encode texts to embeddings.

        Args:
            texts: List of text strings

        Returns:
            Text embeddings array
        """
        if not texts:
            return np.zeros((0, self.embedding_dim))

        with torch.no_grad():
            inputs = self.processor(text=texts, return_tensors="pt", padding=True, truncation=True)
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            outputs = self.model.get_text_features(**inputs)
            embeddings = outputs / outputs.norm(dim=-1, keepdim=True)

        return embeddings.cpu().numpy()


# ============================================================================
# Node2Vec Encoder
# ============================================================================

class Node2VecEncoder:
    """
    Node2Vec encoder for network structure embeddings.
    """

    def __init__(self, config: STAPv4Config):
        """
        Initialize Node2Vec encoder.

        Args:
            config: STAP v4.0 configuration
        """
        self.config = config
        self.graph = nx.Graph()
        self.model = None
        self.embeddings = {}

        if Node2VecModel is None:
            logger.warning("node2vec package not installed. Network embeddings will be disabled.")

        logger.info("Node2Vec encoder initialized")

    def add_edge(self, user1: str, user2: str, weight: float = 1.0):
        """
        Add edge to the social graph.

        Args:
            user1: First user ID
            user2: Second user ID
            weight: Edge weight
        """
        self.graph.add_edge(user1, user2, weight=weight)

    def add_edges(self, edges: List[Tuple[str, str, float]]):
        """
        Add multiple edges to the graph.

        Args:
            edges: List of (user1, user2, weight) tuples
        """
        for user1, user2, weight in edges:
            self.add_edge(user1, user2, weight)

    def train(self):
        """
        Train Node2Vec model on the current graph.
        """
        if Node2VecModel is None:
            logger.warning("node2vec package not available")
            return

        if self.graph.number_of_nodes() < 2:
            logger.warning("Insufficient nodes for Node2Vec training")
            return

        logger.info(f"Training Node2Vec on graph with {self.graph.number_of_nodes()} nodes")

        node2vec = Node2VecModel(
            self.graph,
            dimensions=self.config.node2vec_dimensions,
            walk_length=self.config.node2vec_walk_length,
            num_walks=self.config.node2vec_num_walks,
            p=self.config.node2vec_p,
            q=self.config.node2vec_q,
            workers=self.config.node2vec_workers,
            quiet=True
        )

        self.model = node2vec.fit(
            window=self.config.node2vec_window_size,
            min_count=1,
            batch_words=4
        )

        # Cache embeddings
        for node in self.graph.nodes():
            try:
                self.embeddings[node] = self.model.wv[str(node)]
            except KeyError:
                self.embeddings[node] = np.zeros(self.config.node2vec_dimensions)

        logger.info(f"Node2Vec training complete: {len(self.embeddings)} embeddings")

    def get_embedding(self, user_id: str) -> np.ndarray:
        """
        Get Node2Vec embedding for a user.

        Args:
            user_id: User identifier

        Returns:
            Node2Vec embedding vector
        """
        if user_id in self.embeddings:
            return self.embeddings[user_id]

        # Return zero vector if not found
        return np.zeros(self.config.node2vec_dimensions)

    def compute_bridge_weight(self, user1: str, user2: str,
                            semantic_positions: Dict[str, np.ndarray]) -> float:
        """
        Compute bridge-aware edge weight based on semantic distance.

        Args:
            user1: First user ID
            user2: Second user ID
            semantic_positions: Dictionary of user semantic coordinates

        Returns:
            Bridge-aware weight
        """
        if user1 not in semantic_positions or user2 not in semantic_positions:
            return 1.0

        pos1 = semantic_positions[user1]
        pos2 = semantic_positions[user2]

        # Semantic distance (normalized)
        distance = np.linalg.norm(pos1 - pos2)

        # Bridge score: higher for medium distances (optimal bridge range [0.3, 0.5])
        bridge_score = np.exp(-((distance - 0.4) ** 2) / 0.1)

        # Apply bridge weight strength
        weight = 1.0 + bridge_score

        return float(weight)


# ============================================================================
# Multi-Modal Mahalanobis Distance (High-Dimensional Space)
# ============================================================================

class Align2World:
    """
    Implements multi-modal Mahalanobis distance for high-dimensional space.
    Combines text, image, network, and engagement embeddings with covariance-aware distance.

    Uses robust low-rank + diagonal covariance estimation (Section 2.1.2):
    - Minimum Covariance Determinant (MCD) for outlier resistance
    - SVD-based low-rank approximation: Œ£ ‚âà U_k U_k^T + D
    - Cholesky decomposition for efficient distance computation
    """

    def __init__(self, config: STAPv4Config):
        """
        Initialize multi-modal Mahalanobis distance calculator.

        Args:
            config: STAP v4.0 configuration
        """
        self.config = config
        self.estimator = None
        self.is_fitted = False

        logger.info("Align2World initialized")

    def fit(self, X_multimodal: np.ndarray):
        """
        Compute covariance matrix from multi-modal embeddings.

        Args:
            X_multimodal: (N, d_h) multi-modal embeddings
        """
        N, d_h = X_multimodal.shape

        if N < 2:
            logger.warning("Insufficient data for covariance estimation")
            return

        # Configure covariance estimation
        cov_config = CovarianceConfig(
            method=self.config.covariance_method,
            rank=self.config.covariance_rank,
            epsilon=self.config.covariance_epsilon,
            mcd_support_fraction=self.config.mcd_support_fraction
        )

        # Fit robust covariance estimator
        self.estimator = RobustCovarianceEstimator(cov_config)
        self.estimator.fit(X_multimodal)

        self.is_fitted = True
        logger.info(f"Covariance fitted: method={self.config.covariance_method}, "
                   f"rank={self.estimator.rank}, dim={d_h}")

    def distance(self, x_i: np.ndarray, x_j: np.ndarray) -> float:
        """
        Compute Mahalanobis distance between two embeddings.

        Args:
            x_i: (d_h,) embedding for user i
            x_j: (d_h,) embedding for user j

        Returns:
            Mahalanobis distance
        """
        if not self.is_fitted:
            return np.linalg.norm(x_i - x_j)

        return self.estimator.align_to_world(x_i, x_j)

    def probability(self, x_i: np.ndarray, x_j: np.ndarray, sigma_i: float) -> float:
        """
        Compute high-dimensional connection probability.

        Args:
            x_i: (d_h,) embedding for user i
            x_j: (d_h,) embedding for user j
            sigma_i: Local bandwidth for user i

        Returns:
            Connection probability
        """
        d = self.distance(x_i, x_j)
        return np.exp(-d**2 / (2 * sigma_i**2))

    def get_covariance(self) -> np.ndarray:
        """Get estimated covariance matrix."""
        if not self.is_fitted:
            return None
        return self.estimator.get_covariance()

    def get_precision(self) -> np.ndarray:
        """Get precision matrix (inverse covariance)."""
        if not self.is_fitted:
            return None
        return self.estimator.get_precision()


# ============================================================================
# Hyperbolic Geometry (Low-Dimensional Space)
# ============================================================================

class Flow2World:
    """
    Implements hyperbolic geometry operations in the Poincar√© ball model.
    """

    def __init__(self, dim: int):
        """
        Initialize hyperbolic space.

        Args:
            dim: Dimension of Poincar√© ball
        """
        self.dim = dim
        self.eps = 1e-7  # Numerical stability

    def project_to_ball(self, y: np.ndarray, max_norm: float = 0.99) -> np.ndarray:
        """
        Project point onto Poincar√© ball.

        Args:
            y: Point to project
            max_norm: Maximum norm (< 1)

        Returns:
            Projected point
        """
        norm = np.linalg.norm(y)
        if norm >= 1.0:
            return (max_norm / (norm + self.eps)) * y
        return y

    def hyperbolic_distance(self, x: torch.Tensor, y: torch.Tensor, 
                           use_taylor: bool = None) -> torch.Tensor:
        """
        Compute stable hyperbolic distance in Poincar√© ball with Taylor approximation.
        
        d_H(x,y) = arccosh(1 + 2¬∑||x-y||¬≤/((1-||x||¬≤)(1-||y||¬≤)))
        
        For points near boundary (||x|| > 1-Œµ), uses Taylor approximation:
        d_H(x,y) ‚âà ||x-y|| + (1/6)||x-y||¬≥
        
        Args:
            x: First embedding (batch_size, d) or (d,)
            y: Second embedding (batch_size, d) or (d,)
            use_taylor: Override config setting for Taylor approximation
        
        Returns:
            Hyperbolic distance (batch_size,) or scalar
        """
        if use_taylor is None:
            use_taylor = self.config.use_taylor_approximation
        
        # Ensure tensors are 2D
        if x.dim() == 1:
            x = x.unsqueeze(0)
        if y.dim() == 1:
            y = y.unsqueeze(0)
        
        # Compute norms
        norm_x = torch.norm(x, dim=-1, keepdim=True)
        norm_y = torch.norm(y, dim=-1, keepdim=True)
        
        # Clamp to stay within Poincar√© ball
        norm_x = torch.clamp(norm_x, max=0.99)
        norm_y = torch.clamp(norm_y, max=0.99)
        
        # Check if near boundary
        taylor_threshold = self.config.taylor_threshold
        near_boundary = (norm_x > (1 - taylor_threshold)) | (norm_y > (1 - taylor_threshold))
        
        # Compute exact hyperbolic distance
        diff_norm_sq = torch.sum((x - y)**2, dim=-1, keepdim=True)
        denom = (1 - norm_x**2) * (1 - norm_y**2)
        denom = torch.clamp(denom, min=1e-15)  # Prevent division by zero
        
        alpha = 1 + 2 * diff_norm_sq / denom
        alpha = torch.clamp(alpha, min=1.0)  # Ensure alpha >= 1
        
        # Logarithmic reformulation for stability
        d_exact = torch.log(alpha + torch.sqrt(alpha**2 - 1))
        
        # Taylor approximation for boundary points
        if use_taylor:
            diff_norm = torch.sqrt(diff_norm_sq + 1e-15)
            d_taylor = diff_norm + (1/6) * diff_norm**3
            
            # Blend based on proximity to boundary
            d = torch.where(near_boundary.squeeze(-1), d_taylor.squeeze(-1), d_exact.squeeze(-1))
        else:
            d = d_exact.squeeze(-1)
        
        return d
    def mobius_addition(self, y: np.ndarray, v: np.ndarray) -> np.ndarray:
        """
        M√∂bius addition in Poincar√© ball.

        Args:
            y: Point in Poincar√© ball
            v: Vector to add

        Returns:
            Result of M√∂bius addition
        """
        y_norm_sq = np.sum(y ** 2)
        v_norm_sq = np.sum(v ** 2)
        y_dot_v = np.dot(y, v)

        numerator = (1 + 2 * y_dot_v + v_norm_sq) * y + (1 - y_norm_sq) * v
        denominator = 1 + 2 * y_dot_v + y_norm_sq * v_norm_sq + self.eps

        result = numerator / denominator
        return self.project_to_ball(result)

    def exponential_map(self, y: np.ndarray, v: np.ndarray) -> np.ndarray:
        """
        Exponential map at point y in direction v.

        Args:
            y: Base point in Poincar√© ball
            v: Tangent vector

        Returns:
            Result of exponential map
        """
        v_norm = np.linalg.norm(v)

        if v_norm < self.eps:
            return y

        # Hyperbolic norm
        y_norm_sq = np.sum(y ** 2)
        lambda_y = 2 / (1 - y_norm_sq + self.eps)

        # Exponential map formula
        v_normalized = v / (v_norm + self.eps)
        tanh_term = np.tanh(lambda_y * v_norm / 2)

        result = self.mobius_addition(y, tanh_term * v_normalized)
        return result


# ============================================================================
# Contrastive Loss for Hyperbolic Space
# ============================================================================

class HyperbolicContrastiveLoss:
    """
    Implements contrastive loss in hyperbolic space.
    """

    def __init__(self, flow_to_world: Flow2World, margin: float = 2.0):
        """
        Initialize contrastive loss.

        Args:
            flow_to_world: Flow2World instance
            margin: Margin for dissimilar pairs
        """
        self.flow_to_world = flow_to_world
        self.margin = margin

    def compute_loss(
        self,
        Y: np.ndarray,
        similar_pairs: List[Tuple[int, int]],
        dissimilar_pairs: List[Tuple[int, int]]
    ) -> float:
        """
        Compute contrastive loss.

        Args:
            Y: (N, d) low-dimensional embeddings
            similar_pairs: List of (i, j) similar pairs
            dissimilar_pairs: List of (i, k) dissimilar pairs

        Returns:
            Contrastive loss value
        """
        loss_similar = 0.0
        for i, j in similar_pairs:
            d_H = self.flow_to_world.hyperbolic_distance(Y[i], Y[j])
            loss_similar += d_H ** 2

        loss_dissimilar = 0.0
        for i, k in dissimilar_pairs:
            d_H = self.flow_to_world.hyperbolic_distance(Y[i], Y[k])
            loss_dissimilar += max(0, self.margin - d_H) ** 2

        total_loss = loss_similar / max(len(similar_pairs), 1) + \
                    loss_dissimilar / max(len(dissimilar_pairs), 1)

        return total_loss

    def compute_gradient(
        self,
        Y: np.ndarray,
        similar_pairs: List[Tuple[int, int]],
        dissimilar_pairs: List[Tuple[int, int]]
    ) -> np.ndarray:
        """
        Compute gradient of contrastive loss.

        Args:
            Y: (N, d) low-dimensional embeddings
            similar_pairs: List of similar pairs
            dissimilar_pairs: List of dissimilar pairs

        Returns:
            Gradient array (N, d)
        """
        N, d = Y.shape
        grad = np.zeros((N, d))

        # Similar pairs gradient
        for i, j in similar_pairs:
            d_H = self.flow_to_world.hyperbolic_distance(Y[i], Y[j])
            if d_H > 1e-10:
                coeff = 2 * d_H
                grad[i] += coeff * (Y[i] - Y[j])
                grad[j] += coeff * (Y[j] - Y[i])

        # Dissimilar pairs gradient
        for i, k in dissimilar_pairs:
            d_H = self.flow_to_world.hyperbolic_distance(Y[i], Y[k])
            if d_H < self.margin:
                coeff = -2 * (self.margin - d_H)
                grad[i] += coeff * (Y[i] - Y[k])
                grad[k] += coeff * (Y[k] - Y[i])

        # Normalize by number of pairs
        grad /= max(len(similar_pairs) + len(dissimilar_pairs), 1)

        return grad


# ============================================================================
# Siamese Neural Network for Bridge-Aware Scoring
# ============================================================================


# ============================================================================
# MOMENTUM CONTRASTIVE LEARNING (MoCo)
# ============================================================================

class MoCoQueue:
    """
    Momentum Contrast queue for robust negative sampling.
    
    Maintains a queue of embeddings updated with momentum for consistent
    negative samples across batches.
    """
    
    def __init__(self, dim: int, K: int = 4096, momentum: float = 0.999):
        """
        Args:
            dim: Embedding dimension
            K: Queue size
            momentum: Momentum coefficient for queue updates
        """
        self.dim = dim
        self.K = K
        self.momentum = momentum
        self.queue = torch.zeros(K, dim)
        self.ptr = 0
    
    def enqueue(self, embeddings: torch.Tensor):
        """
        Add embeddings to queue with FIFO policy.
        
        Args:
            embeddings: Batch of embeddings (batch_size, dim)
        """
        batch_size = embeddings.shape[0]
        
        # Ensure we don't overflow
        if batch_size > self.K:
            embeddings = embeddings[:self.K]
            batch_size = self.K
        
        # Replace oldest embeddings
        end_ptr = self.ptr + batch_size
        if end_ptr <= self.K:
            self.queue[self.ptr:end_ptr] = embeddings.detach()
        else:
            # Wrap around
            overflow = end_ptr - self.K
            self.queue[self.ptr:] = embeddings[:self.K - self.ptr].detach()
            self.queue[:overflow] = embeddings[self.K - self.ptr:].detach()
        
        self.ptr = end_ptr % self.K
    
    def get_negatives(self, n: int = None):
        """
        Sample negatives from queue.
        
        Args:
            n: Number of negatives to sample (None = all)
        
        Returns:
            Negative embeddings (n, dim) or (K, dim)
        """
        if n is None:
            return self.queue
        else:
            indices = torch.randperm(self.K)[:n]
            return self.queue[indices]
    
    def to(self, device):
        """Move queue to device"""
        self.queue = self.queue.to(device)
        return self


# ============================================================================
# HYPERBOLIC TRIPLET LOSS
# ============================================================================

class HyperbolicTripletLoss(nn.Module):
    """
    Triplet loss in hyperbolic space for bridge validation.
    
    L_triplet = Œ£ max(0, margin + d_H(anchor, positive) - d_H(anchor, negative))
    
    Encourages:
    - Small distance between anchor and positive (similar users)
    - Large distance between anchor and negative (dissimilar users)
    """
    
    def __init__(self, margin: float = 1.0):
        """
        Args:
            margin: Minimum separation between positive and negative distances
        """
        super().__init__()
        self.margin = margin
    
    def forward(self, anchor: torch.Tensor, positive: torch.Tensor, 
                negative: torch.Tensor, hyperbolic_dist_fn):
        """
        Compute triplet loss.
        
        Args:
            anchor: Anchor embeddings (batch_size, dim)
            positive: Positive embeddings (batch_size, dim)
            negative: Negative embeddings (batch_size, dim)
            hyperbolic_dist_fn: Function to compute hyperbolic distance
        
        Returns:
            Triplet loss (scalar)
        """
        # Compute distances
        dist_pos = hyperbolic_dist_fn(anchor, positive)
        dist_neg = hyperbolic_dist_fn(anchor, negative)
        
        # Triplet loss with margin
        loss = torch.relu(self.margin + dist_pos - dist_neg)
        
        return torch.mean(loss)
    
    def mine_hard_negatives(self, anchor: torch.Tensor, negatives: torch.Tensor,
                           hyperbolic_dist_fn, k: int = 10):
        """
        Mine hard negatives (closest negatives to anchor).
        
        Args:
            anchor: Anchor embedding (1, dim)
            negatives: Candidate negatives (N, dim)
            hyperbolic_dist_fn: Distance function
            k: Number of hard negatives to select
        
        Returns:
            Hard negative embeddings (k, dim)
        """
        # Compute distances to all negatives
        distances = torch.stack([
            hyperbolic_dist_fn(anchor, neg.unsqueeze(0))
            for neg in negatives
        ])
        
        # Select k closest (hardest) negatives
        _, indices = torch.topk(distances, k, largest=False)
        
        return negatives[indices]




# ============================================================================
# MANIFOLD REGULARIZERS FOR HYPERBOLIC SPACE
# ============================================================================

class ManifoldRegularizers(nn.Module):
    """
    Collection of manifold-aware regularizers for stable hyperbolic optimization.
    
    Implements 5 regularizers from Bu et al. (2025), Nickel & Kiela (2017), 
    Chami et al. (2019), and related work.
    """
    
    def __init__(self, config: 'STAPv4Config'):
        super().__init__()
        self.config = config
        self.gamma = config.gamma if hasattr(config, 'gamma') else 2.0
    
    def curvature_gradient_regularizer(self, y: torch.Tensor, grad_y: torch.Tensor):
        """
        R_curv = Œ£_i ||‚àáy_i||¬≤ ¬∑ (1 - ||y_i||¬≤)^(-2)
        
        Prevents exploding gradients near boundary by accounting for metric tensor scaling.
        
        Args:
            y: Embeddings in Poincar√© ball (N, d)
            grad_y: Gradients w.r.t. y (N, d)
        
        Returns:
            Curvature-sensitive gradient penalty (scalar)
        """
        norm_y = torch.norm(y, dim=-1, keepdim=True)
        norm_y = torch.clamp(norm_y, max=0.99)  # Prevent division by zero
        
        # Metric tensor factor: g(y) = (2 / (1 - ||y||¬≤))¬≤
        metric_factor = 1 / ((1 - norm_y**2 + 1e-15)**2)
        
        # Gradient norm squared
        grad_norm_sq = torch.sum(grad_y**2, dim=-1, keepdim=True)
        
        # Weighted gradient penalty
        penalty = grad_norm_sq * metric_factor
        
        return torch.mean(penalty)
    
    def taylor_consistency_penalty(self, y_i: torch.Tensor, y_j: torch.Tensor,
                                   hyperbolic_dist_fn):
        """
        R_Taylor = Œ£_{i,j} |d_H(y_i, y_j) - d_Taylor(y_i, y_j)|¬≤
        
        Minimizes discrepancy between exact hyperbolic distance and Taylor approximation.
        
        Args:
            y_i: First set of embeddings (N, d)
            y_j: Second set of embeddings (N, d)
            hyperbolic_dist_fn: Function that computes hyperbolic distance
        
        Returns:
            Taylor consistency penalty (scalar)
        """
        # Compute exact distance
        d_exact = hyperbolic_dist_fn(y_i, y_j, use_taylor=False)
        
        # Compute Taylor approximation
        d_taylor = hyperbolic_dist_fn(y_i, y_j, use_taylor=True)
        
        # Minimize discrepancy
        penalty = (d_exact - d_taylor)**2
        
        return torch.mean(penalty)
    
    def manifold_conformality_regularizer(self, y: torch.Tensor, grad_y: torch.Tensor):
        """
        R_conf = Œ£_i (||y_i|| - ||‚àáy_i||)¬≤
        
        Maintains conformal structure by coupling radial position with gradient magnitude.
        
        Args:
            y: Embeddings (N, d)
            grad_y: Gradients (N, d)
        
        Returns:
            Conformality penalty (scalar)
        """
        norm_y = torch.norm(y, dim=-1)
        grad_norm = torch.norm(grad_y, dim=-1)
        
        penalty = (norm_y - grad_norm)**2
        
        return torch.mean(penalty)
    
    def contrastive_radius_margin_regularizer(self, y_similar: torch.Tensor,
                                             y_dissimilar: torch.Tensor):
        """
        R_radius = Œ£_{(i,j)‚ààS} (||y_i|| - ||y_j||)¬≤ 
                   - Œ£_{(i,k)‚ààD} max(0, Œ≥ - |||y_i|| - ||y_k|||)
        
        Encourages similar users to occupy similar radial positions (hierarchical clustering).
        
        Args:
            y_similar: Pairs of similar embeddings (N, 2, d)
            y_dissimilar: Pairs of dissimilar embeddings (M, 2, d)
        
        Returns:
            Radius margin penalty (scalar)
        """
        # Similar pairs: minimize radial difference
        if y_similar.shape[0] > 0:
            norm_sim_i = torch.norm(y_similar[:, 0], dim=-1)
            norm_sim_j = torch.norm(y_similar[:, 1], dim=-1)
            loss_similar = torch.mean((norm_sim_i - norm_sim_j)**2)
        else:
            loss_similar = torch.tensor(0.0, device=y_similar.device)
        
        # Dissimilar pairs: maximize radial difference (with margin)
        if y_dissimilar.shape[0] > 0:
            norm_dis_i = torch.norm(y_dissimilar[:, 0], dim=-1)
            norm_dis_k = torch.norm(y_dissimilar[:, 1], dim=-1)
            radial_diff = torch.abs(norm_dis_i - norm_dis_k)
            loss_dissimilar = torch.mean(torch.relu(self.gamma - radial_diff))
        else:
            loss_dissimilar = torch.tensor(0.0, device=y_dissimilar.device)
        
        return loss_similar - loss_dissimilar
    
    def manifold_entropy_regularizer(self, y: torch.Tensor, n_bins: int = 36):
        """
        R_entropy = -H(Œ∏) where H is angular entropy
        
        Prevents mode collapse via angular diversity.
        
        Args:
            y: Embeddings (N, d)
            n_bins: Number of angular bins
        
        Returns:
            Negative entropy (to minimize = maximize entropy)
        """
        # For high-dimensional space, use PCA to project to 2D for angle computation
        if y.shape[1] > 2:
            # Simple projection to first 2 dimensions
            y_2d = y[:, :2]
        else:
            y_2d = y
        
        # Compute angles
        angles = torch.atan2(y_2d[:, 1], y_2d[:, 0])
        
        # Normalize to [0, 2œÄ]
        angles = (angles + 2 * np.pi) % (2 * np.pi)
        
        # Compute histogram
        hist = torch.histc(angles, bins=n_bins, min=0, max=2*np.pi)
        
        # Compute probabilities
        probs = hist / (torch.sum(hist) + 1e-15)
        probs = torch.clamp(probs, min=1e-15)
        
        # Compute entropy
        entropy = -torch.sum(probs * torch.log(probs))
        
        # Return negative entropy (minimize = maximize entropy)
        return -entropy
    
    def compute_all_regularizers(self, y: torch.Tensor, grad_y: torch.Tensor,
                                 y_similar: torch.Tensor, y_dissimilar: torch.Tensor,
                                 hyperbolic_dist_fn):
        """
        Compute all 5 manifold regularizers with configured weights.
        
        Returns:
            Dictionary of individual regularizer values and total weighted loss
        """
        regularizers = {}
        
        # 1. Curvature-sensitive gradient
        if self.config.lambda_curvature > 0:
            regularizers['curvature'] = self.curvature_gradient_regularizer(y, grad_y)
        else:
            regularizers['curvature'] = torch.tensor(0.0, device=y.device)
        
        # 2. Taylor consistency
        if self.config.lambda_taylor_consistency > 0 and y.shape[0] > 1:
            # Sample pairs for efficiency
            n_pairs = min(100, y.shape[0] - 1)
            indices_i = torch.randint(0, y.shape[0], (n_pairs,))
            indices_j = torch.randint(0, y.shape[0], (n_pairs,))
            regularizers['taylor'] = self.taylor_consistency_penalty(
                y[indices_i], y[indices_j], hyperbolic_dist_fn
            )
        else:
            regularizers['taylor'] = torch.tensor(0.0, device=y.device)
        
        # 3. Manifold conformality
        if self.config.lambda_conformality > 0:
            regularizers['conformality'] = self.manifold_conformality_regularizer(y, grad_y)
        else:
            regularizers['conformality'] = torch.tensor(0.0, device=y.device)
        
        # 4. Contrastive radius margin
        if self.config.lambda_radius > 0:
            regularizers['radius'] = self.contrastive_radius_margin_regularizer(
                y_similar, y_dissimilar
            )
        else:
            regularizers['radius'] = torch.tensor(0.0, device=y.device)
        
        # 5. Manifold entropy
        if self.config.lambda_entropy > 0:
            regularizers['entropy'] = self.manifold_entropy_regularizer(y)
        else:
            regularizers['entropy'] = torch.tensor(0.0, device=y.device)
        
        # Compute weighted total
        total = (
            self.config.lambda_curvature * regularizers['curvature'] +
            self.config.lambda_taylor_consistency * regularizers['taylor'] +
            self.config.lambda_conformality * regularizers['conformality'] +
            self.config.lambda_radius * regularizers['radius'] +
            self.config.lambda_entropy * regularizers['entropy']
        )
        
        regularizers['total'] = total
        
        return regularizers




class SiameseNetwork(nn.Module):
    """
    Siamese neural network for learning bridge success probability.
    """

    def __init__(self, d_low: int, d_context: int, d_hidden: int, d_output: int):
        """
        Initialize Siamese network.

        Args:
            d_low: Dimension of low-dimensional embeddings
            d_context: Dimension of context vectors
            d_hidden: Hidden layer dimension
            d_output: Output dimension
        """
        super(SiameseNetwork, self).__init__()

        # Input dimension: low-dim embedding + context
        d_input = d_low + d_context

        # Shared tower
        self.tower = nn.Sequential(
            nn.Linear(d_input, d_hidden),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(d_hidden, d_hidden),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(d_hidden, d_output)
        )

        # Bridge prediction head
        self.bridge_head = nn.Sequential(
            nn.Linear(d_output * 2, d_hidden),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(d_hidden, 1),
            nn.Sigmoid()
        )

    def forward(self, y_i: torch.Tensor, context_i: torch.Tensor,
                y_j: torch.Tensor, context_j: torch.Tensor) -> torch.Tensor:
        """
        Forward pass.

        Args:
            y_i: Low-dimensional embedding for user i (batch_size, d_low)
            context_i: Context vector for user i (batch_size, d_context)
            y_j: Low-dimensional embedding for user j (batch_size, d_low)
            context_j: Context vector for user j (batch_size, d_context)

        Returns:
            Bridge probability (batch_size, 1)
        """
        # Concatenate embeddings and context
        x_i = torch.cat([y_i, context_i], dim=1)
        x_j = torch.cat([y_j, context_j], dim=1)

        # Pass through shared tower
        h_i = self.tower(x_i)
        h_j = self.tower(x_j)

        # Concatenate representations
        h_pair = torch.cat([h_i, h_j], dim=1)

        # Predict bridge probability
        bridge_prob = self.bridge_head(h_pair)

        return bridge_prob

    def bridge_score(self, y_i: np.ndarray, context_i: np.ndarray,
                    y_j: np.ndarray, context_j: np.ndarray,
                    device: str = 'cpu') -> float:
        """
        Compute bridge score for a single pair.

        Args:
            y_i: Low-dimensional embedding for user i
            context_i: Context vector for user i
            y_j: Low-dimensional embedding for user j
            context_j: Context vector for user j
            device: Device for computation

        Returns:
            Bridge score
        """
        self.eval()
        with torch.no_grad():
            y_i_t = torch.from_numpy(y_i).float().unsqueeze(0).to(device)
            context_i_t = torch.from_numpy(context_i).float().unsqueeze(0).to(device)
            y_j_t = torch.from_numpy(y_j).float().unsqueeze(0).to(device)
            context_j_t = torch.from_numpy(context_j).float().unsqueeze(0).to(device)

            score = self.forward(y_i_t, context_i_t, y_j_t, context_j_t)

        return float(score.item())


# ============================================================================
# STAP v4.0 Preprocessing Layer (GitHub-Friendly Version)
# ============================================================================

class STAPv4PreprocessingLayer:
    """
    STAP v4.0 Preprocessing Layer with integrated multi-modal support:
    1. Multi-Modal Mahalanobis Distance (high-dimensional)
    2. Hyperbolic Geometry with Contrastive Loss (low-dimensional)
    3. Siamese Neural Network (bridge-aware scoring)
    4. CLIP Integration (image-text embeddings)
    5. Node2Vec Integration (network structure embeddings)
    """

    def __init__(self, config: Optional[STAPv4Config] = None):
        """
        Initialize STAP v4.0 preprocessing layer securely.

        Args:
            config: STAP v4.0 configuration
        """
        self.config = config or STAPv4Config()

        # Initialize text embedding model
        logger.info(f"Loading text embedding model: {self.config.text_embedding_model}")
        self.text_embedding_model = SentenceTransformer(self.config.text_embedding_model)
        self.text_embedding_dim = self.text_embedding_model.get_sentence_embedding_dimension()

        # Initialize CLIP encoder with secure token handling
        self.clip_encoder = None
        if self.config.use_clip:
            try:
                self.clip_encoder = CLIPEncoder(
                    self.config.clip_model,
                    self.config.device,
                    hf_token=HF_TOKEN  # Pass the securely retrieved token
                )
                self.image_embedding_dim = self.clip_encoder.embedding_dim
            except Exception as e:
                logger.warning(f"Failed to initialize CLIP encoder: {e}")
                self.config.use_clip = False
                self.image_embedding_dim = 512
        else:
            self.image_embedding_dim = 512

        # Initialize Node2Vec encoder
        self.node2vec_encoder = None
        if self.config.use_node2vec:
            self.node2vec_encoder = Node2VecEncoder(self.config)
            self.network_embedding_dim = self.config.node2vec_dimensions
        else:
            self.network_embedding_dim = 128

        # Engagement embedding dimension
        self.engagement_embedding_dim = 32

        # Total high-dimensional size
        self.high_dim_size = (
            self.text_embedding_dim +
            self.image_embedding_dim +
            self.network_embedding_dim +
            self.engagement_embedding_dim
        )

        # Storage for global state
        self.high_dim_embeddings = []  # Multi-modal high-dimensional embeddings
        self.low_dim_embeddings = []   # Hyperbolic low-dimensional embeddings
        self.user_id_map = {}
        self.index_to_user_id = {}

        # Initialize components
        self.align_to_world = Align2World(self.config)
        self.flow_to_world = Flow2World(self.config.target_dim)
        self.contrastive_loss = HyperbolicContrastiveLoss(
            self.flow_to_world,
            margin=self.config.hyperbolic_margin
        )

        # Siamese network (will be loaded if available)
        self.siamese_network = None
        if self.config.use_siamese:
            self.siamese_network = SiameseNetwork(
                d_low=self.config.target_dim,
                d_context=self.config.context_dim,
                d_hidden=self.config.siamese_hidden_dim,
                d_output=self.config.siamese_output_dim
            ).to(self.config.device)

        # HNSW index
        self.hnsw_index = None
        self.is_fitted = False

        # Cache
        self.P_high = None
        self.local_sigmas = None

        logger.info(f"STAP v4.0 initialized: {self.high_dim_size}D -> {self.config.target_dim}D (hyperbolic={self.config.use_hyperbolic})")

    def generate_semantic_coordinate(
        self,
        user_id: str,
        user_corpus: List[str],
        engagement_patterns: Dict,
        image_data: Optional[List] = None,
        network_connections: Optional[List[Tuple[str, str, float]]] = None
    ) -> Tuple[np.ndarray, float]:
        """
        Generate semantic coordinate using STAP v4.0 framework.

        Args:
            user_id: Unique user identifier
            user_corpus: List of user's text content
            engagement_patterns: Dict with interaction data (likes, shares, comments, etc.)
            image_data: Optional list of images (URLs, PIL Images, or numpy arrays)
            network_connections: Optional list of (user1, user2, weight) tuples

        Returns:
            Tuple of (semantic_coordinate, confidence_score)
        """
        # Step 1: Generate multi-modal high-dimensional embedding
        multimodal_embedding = self._generate_multimodal_embedding(
            user_corpus,
            engagement_patterns,
            image_data,
            network_connections
        )

        # Step 2: Add to global collection
        if user_id in self.user_id_map:
            idx = self.user_id_map[user_id]
            self.high_dim_embeddings[idx] = multimodal_embedding
        else:
            idx = len(self.high_dim_embeddings)
            self.high_dim_embeddings.append(multimodal_embedding)
            self.user_id_map[user_id] = idx
            self.index_to_user_id[idx] = user_id

            # Initialize low-dimensional embedding in Poincar√© ball
            initial_embedding = np.random.randn(self.config.target_dim) * 0.01
            initial_embedding = self.flow_to_world.project_to_ball(initial_embedding)
            self.low_dim_embeddings.append(initial_embedding)

        # Step 3: Update Node2Vec graph if network connections provided
        if network_connections and self.node2vec_encoder:
            self.node2vec_encoder.add_edges(network_connections)

        # Step 4: Refit STAP if enough users
        if len(self.high_dim_embeddings) >= self.config.n_neighbors:
            if not self.is_fitted or len(self.high_dim_embeddings) % 10 == 0:
                self._fit_stap_v4_projection()

        # Step 5: Get optimized coordinate
        semantic_coordinate = self.low_dim_embeddings[idx]

        # Step 6: Calculate confidence
        confidence = self._calculate_confidence(
            user_corpus,
            engagement_patterns
        )

        # Step 7: Update HNSW index
        if self.is_fitted:
            self._update_hnsw_index()

        return semantic_coordinate, confidence

    def _generate_multimodal_embedding(
        self,
        text_corpus: List[str],
        engagement_patterns: Dict,
        image_data: Optional[List],
        network_connections: Optional[List[Tuple[str, str, float]]]
    ) -> np.ndarray:

        """Generate multi-modal embedding by fusing text, image, network, and engagement data.

        Args:
            text_corpus: List of text strings
            engagement_patterns: Dictionary of engagement metrics
            image_data: Optional image data
            network_connections: Optional network connections

        Returns:
            Multi-modal embedding vector
        """
        # Text embedding
        text_emb = self._generate_text_embeddings(text_corpus)

        # Image embedding using CLIP
        if image_data is not None and len(image_data) > 0 and self.clip_encoder:
            image_embeddings = self.clip_encoder.encode_images(image_data)
            if len(image_embeddings) > 0:
                image_emb = np.mean(image_embeddings, axis=0)
            else:
                image_emb = np.zeros(self.image_embedding_dim)
        else:
            image_emb = np.zeros(self.image_embedding_dim)

        # Network embedding using Node2Vec
        # Note: This requires the user to be in the graph and the model to be trained
        network_emb = np.zeros(self.network_embedding_dim)

        # Engagement embedding
        engagement_emb = self._generate_engagement_embedding(engagement_patterns)

        # Weighted concatenation
        multimodal_emb = np.con


# ============================================================================
# BRIDGE2WORLD: SIAMESE TRANSFORMER FOR BRIDGE PREDICTION
# ============================================================================

# ============================================================================
# 1. SIAMESE TRANSFORMER ARCHITECTURE
# ============================================================================

class Bridge2World(nn.Module):
    """
    Bridge2World: Siamese Transformer for bridge prediction.

    Args:
        vocab_size: Size of vocabulary
        hidden_dim: Hidden dimension size (default: 384)
        num_heads: Number of attention heads (default: 8)
        num_layers: Number of transformer layers (default: 6)
        max_seq_length: Maximum sequence length (default: 512)
        dropout: Dropout rate (default: 0.1)
        activation: Activation function (default: 'gelu')
        layer_norm_eps: Layer normalization epsilon (default: 1e-12)

    Returns:
        Tuple of (similarity_score, attention_weights) when return_attn=True
    """

    def __init__(
        self,
        vocab_size: int,
        hidden_dim: int = 384,
        num_heads: int = 8,
        num_layers: int = 6,
        max_seq_length: int = 512,
        dropout: float = 0.1,
        activation: str = 'gelu',
        layer_norm_eps: float = 1e-12
    ):
        super(SiameseTransformer, self).__init__()

        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.max_seq_length = max_seq_length

        # Embedding layers
        self.token_embedding = nn.Embedding(vocab_size, hidden_dim)
        self.position_embedding = nn.Embedding(max_seq_length, hidden_dim)
        self.dropout = nn.Dropout(dropout)
        self.layer_norm = nn.LayerNorm(hidden_dim, eps=layer_norm_eps)

        # Transformer encoder (shared between both branches)
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_dim,
            nhead=num_heads,
            dim_feedforward=hidden_dim * 4,
            dropout=dropout,
            activation=activation,
            layer_norm_eps=layer_norm_eps,
            batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(
            encoder_layer,
            num_layers=num_layers
        )

        # Output projection
        self.output_projection = nn.Linear(hidden_dim, hidden_dim)
        self.output_activation = nn.Tanh()

        # Initialize weights
        self.apply(self._init_weights)

    def _init_weights(self, module):
        """Initialize weights for transformer layers"""
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
        elif isinstance(module, nn.LayerNorm):
            torch.nn.init.zeros_(module.bias)
            torch.nn.init.ones_(module.weight)

    def forward(
        self,
        input_ids1: torch.Tensor,
        attention_mask1: torch.Tensor,
        input_ids2: torch.Tensor,
        attention_mask2: torch.Tensor,
        return_attn: bool = False
    ) -> Union[torch.Tensor, Tuple[torch.Tensor, Dict]]:
        """
        Forward pass for Siamese Transformer.

        Args:
            input_ids1: Token IDs for first sequence [batch_size, seq_len]
            attention_mask1: Attention mask for first sequence [batch_size, seq_len]
            input_ids2: Token IDs for second sequence [batch_size, seq_len]
            attention_mask2: Attention mask for second sequence [batch_size, seq_len]
            return_attn: Whether to return attention weights

        Returns:
            similarity_scores or (similarity_scores, attention_weights)
        """
        # Encode both sequences
        embedding1, attn_weights1 = self._encode_sequence(
            input_ids1, attention_mask1, return_attn
        )
        embedding2, attn_weights2 = self._encode_sequence(
            input_ids2, attention_mask2, return_attn
        )

        # Compute cosine similarity
        similarity_scores = F.cosine_similarity(embedding1, embedding2, dim=-1)

        if return_attn:
            attention_weights = {
                'sequence1': attn_weights1,
                'sequence2': attn_weights2
            }
            return similarity_scores, attention_weights

        return similarity_scores

    def _encode_sequence(
        self,
        input_ids: torch.Tensor,
        attention_mask: torch.Tensor,
        return_attn: bool = False
    ) -> Union[torch.Tensor, Tuple[torch.Tensor, list]]:
        """
        Encode a single sequence through transformer.

        Args:
            input_ids: Token IDs [batch_size, seq_len]
            attention_mask: Attention mask [batch_size, seq_len]
            return_attn: Whether to return attention weights

        Returns:
            Sequence embedding or (embedding, attention_weights)
        """
        batch_size, seq_len = input_ids.shape

        # Create position IDs
        position_ids = torch.arange(seq_len, dtype=torch.long, device=input_ids.device)
        position_ids = position_ids.unsqueeze(0).expand(batch_size, -1)

        # Get token and position embeddings
        token_embeddings = self.token_embedding(input_ids)
        position_embeddings = self.position_embedding(position_ids)

        # Combine embeddings
        embeddings = token_embeddings + position_embeddings
        embeddings = self.layer_norm(embeddings)
        embeddings = self.dropout(embeddings)

        # Transformer encoding
        if return_attn:
            # Store attention weights
            attn_weights = []
            def hook_fn(module, input, output):
                attn_weights.append(output[1])  # output[1] contains attention weights

            hooks = []
            for layer in self.transformer_encoder.layers:
                hook = layer.self_attn.register_forward_hook(hook_fn)
                hooks.append(hook)

            # Forward pass
            encoded = self.transformer_encoder(
                embeddings,
                src_key_padding_mask=~attention_mask.bool()
            )

            # Remove hooks
            for hook in hooks:
                hook.remove()

        else:
            encoded = self.transformer_encoder(
                embeddings,
                src_key_padding_mask=~attention_mask.bool()
            )
            attn_weights = None

        # Mean pooling (mask-aware)
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(encoded.size()).float()
        sum_embeddings = torch.sum(encoded * input_mask_expanded, 1)
        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)
        pooled_embeddings = sum_embeddings / sum_mask

        # Project and activate
        projected = self.output_projection(pooled_embeddings)
        final_embedding = self.output_activation(projected)

        if return_attn:
            return final_embedding, attn_weights

        return final_embedding

# ============================================================================
# 2. MODULAR SIMILARITY HEAD
# ============================================================================

class SimilarityHead(nn.Module):
    """
    Modular similarity head supporting multiple similarity metrics.

    Args:
        hidden_dim: Hidden dimension size
        metric_type: Similarity metric type ('cosine', 'euclidean', 'bilinear', 'manhattan')
        temperature: Temperature for cosine similarity (default: 0.05)
        margin: Margin for distance-based metrics (default: 1.0)
    """

    def __init__(
        self,
        hidden_dim: int,
        metric_type: str = 'cosine',
        temperature: float = 0.05,
        margin: float = 1.0
    ):
        super(ModularSimilarityHead, self).__init__()

        self.hidden_dim = hidden_dim
        self.metric_type = metric_type
        self.temperature = temperature
        self.margin = margin

        if metric_type == 'bilinear':
            self.bilinear = nn.Bilinear(hidden_dim, hidden_dim, 1)
            nn.init.xavier_uniform_(self.bilinear.weight)
            nn.init.zeros_(self.bilinear.bias)
        elif metric_type == 'projected':
            self.projection = nn.Sequential(
                nn.Linear(hidden_dim * 2, hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(hidden_dim, 1),
                nn.Sigmoid()
            )

    def forward(
        self,
        embedding1: torch.Tensor,
        embedding2: torch.Tensor
    ) -> torch.Tensor:
        """
        Compute similarity between two embeddings.

        Args:
            embedding1: First embedding [batch_size, hidden_dim]
            embedding2: Second embedding [batch_size, hidden_dim]

        Returns:
            Similarity scores [batch_size]
        """

        if self.metric_type == 'cosine':
            similarity = F.cosine_similarity(embedding1, embedding2, dim=-1)
            # Apply temperature scaling
            similarity = similarity / self.temperature
            return torch.sigmoid(similarity)

        elif self.metric_type == 'euclidean':
            distance = F.pairwise_distance(embedding1, embedding2, p=2)
            similarity = 1.0 / (1.0 + distance)
            return similarity

        elif self.metric_type == 'manhattan':
            distance = F.pairwise_distance(embedding1, embedding2, p=1)
            similarity = 1.0 / (1.0 + distance)
            return similarity

        elif self.metric_type == 'bilinear':
            similarity = self.bilinear(embedding1, embedding2).squeeze(-1)
            return torch.sigmoid(similarity)

        elif self.metric_type == 'projected':
            combined = torch.cat([embedding1, embedding2], dim=-1)
            similarity = self.projection(combined).squeeze(-1)
            return similarity

        else:
            raise ValueError(f"Unsupported metric type: {self.metric_type}")

class EnhancedBridge2World(SiameseTransformer):
    """
    Enhanced Bridge2World with modular similarity head.
    """

    def __init__(
        self,
        vocab_size: int,
        hidden_dim: int = 384,
        num_heads: int = 8,
        num_layers: int = 6,
        max_seq_length: int = 512,
        dropout: float = 0.1,
        activation: str = 'gelu',
        layer_norm_eps: float = 1e-12,
        similarity_metric: str = 'cosine',
        temperature: float = 0.05
    ):
        super().__init__(
            vocab_size, hidden_dim, num_heads, num_layers,
            max_seq_length, dropout, activation, layer_norm_eps
        )

        self.similarity_head = SimilarityHead(
            hidden_dim=hidden_dim,
            metric_type=similarity_metric,
            temperature=temperature
        )

    def forward(
        self,
        input_ids1: torch.Tensor,
        attention_mask1: torch.Tensor,
        input_ids2: torch.Tensor,
        attention_mask2: torch.Tensor,
        return_attn: bool = False
    ) -> Union[torch.Tensor, Tuple[torch.Tensor, Dict]]:
        """
        Forward pass with modular similarity head.
        """
        # Encode both sequences
        embedding1, attn_weights1 = self._encode_sequence(
            input_ids1, attention_mask1, return_attn
        )
        embedding2, attn_weights2 = self._encode_sequence(
            input_ids2, attention_mask2, return_attn
        )

        # Compute similarity using modular head
        similarity_scores = self.similarity_head(embedding1, embedding2)

        if return_attn:
            attention_weights = {
                'sequence1': attn_weights1,
                'sequence2': attn_weights2
            }
            return similarity_scores, attention_weights

        return similarity_scores

# ============================================================================
# 3. ATTENTION VISUALIZATION TOOLS
# ============================================================================

class AttentionVisualizer:
    """
    Visualization tools for transformer attention weights.
    """

    @staticmethod
    def plot_attention_heatmap(
        attention_weights: torch.Tensor,
        tokens: list,
        layer: int = 0,
        head: int = 0,
        figsize: Tuple[int, int] = (12, 8),
        cmap: str = 'viridis'
    ):
        """
        Plot attention heatmap for specific layer and head.

        Args:
            attention_weights: Attention weights tensor [num_layers, num_heads, seq_len, seq_len]
            tokens: List of tokens for axis labels
            layer: Layer index to visualize
            head: Head index to visualize
            figsize: Figure size
            cmap: Colormap for heatmap
        """
        if isinstance(attention_weights, list):
            # Convert list of attention weights to tensor
            attn_tensor = torch.stack(attention_weights)
        else:
            attn_tensor = attention_weights

        # Get specific layer and head
        if attn_tensor.dim() == 4:
            attn_data = attn_tensor[layer, head].cpu().detach().numpy()
        else:
            attn_data = attn_tensor.cpu().detach().numpy()

        fig, ax = plt.subplots(figsize=figsize)
        im = ax.imshow(attn_data, cmap=cmap, aspect='auto')

        # Set labels
        ax.set_xticks(range(len(tokens)))
        ax.set_yticks(range(len(tokens)))
        ax.set_xticklabels(tokens, rotation=45, ha='right')
        ax.set_yticklabels(tokens)

        # Add colorbar
        plt.colorbar(im, ax=ax)
        ax.set_title(f'Attention Weights - Layer {layer}, Head {head}')
        ax.set_xlabel('Key Position')
        ax.set_ylabel('Query Position')

        plt.tight_layout()
        return fig

    @staticmethod
    def plot_multihead_attention(
        attention_weights: torch.Tensor,
        tokens: list,
        layer: int = 0,
        figsize: Tuple[int, int] = (20, 16)
    ):
        """
        Plot attention weights for all heads in a layer.

        Args:
            attention_weights: Attention weights tensor
            tokens: List of tokens
            layer: Layer index
            figsize: Figure size
        """
        if isinstance(attention_weights, list):
            attn_tensor = torch.stack(attention_weights)
        else:
            attn_tensor = attention_weights

        num_heads = attn_tensor.shape[1]

        fig, axes = plt.subplots(
            nrows=int(math.ceil(num_heads / 4)),
            ncols=4,
            figsize=figsize
        )
        axes = axes.flatten() if num_heads > 1 else [axes]

        for head in range(num_heads):
            ax = axes[head]
            attn_data = attn_tensor[layer, head].cpu().detach().numpy()

            im = ax.imshow(attn_data, cmap='viridis', aspect='auto')
            ax.set_title(f'Head {head}')

            if head >= num_heads - 4 or head == num_heads - 1:
                ax.set_xticks(range(len(tokens)))
                ax.set_xticklabels(tokens, rotation=45, ha='right')
            else:
                ax.set_xticks([])

            if head % 4 == 0:
                ax.set_yticks(range(len(tokens)))
                ax.set_yticklabels(tokens)
            else:
                ax.set_yticks([])

        # Remove empty subplots
        for head in range(num_heads, len(axes)):
            fig.delaxes(axes[head])

        plt.tight_layout()
        plt.colorbar(im, ax=axes, location='right', shrink=0.8)
        return fig

def visualize_attention(
    model: SiameseTransformer,
    input_ids: torch.Tensor,
    attention_mask: torch.Tensor,
    tokenizer,
    layer: int = 0,
    head: Optional[int] = None
):
    """
    Visualize attention weights for a single sequence.

    Args:
        model: SiameseTransformer instance
        input_ids: Input token IDs
        attention_mask: Attention mask
        tokenizer: Tokenizer for decoding tokens
        layer: Layer index to visualize
        head: Specific head to visualize (if None, show all heads)
    """
    # Encode with attention return
    with torch.no_grad():
        embedding, attn_weights = model._encode_sequence(
            input_ids.unsqueeze(0) if input_ids.dim() == 1 else input_ids,
            attention_mask.unsqueeze(0) if attention_mask.dim() == 1 else attention_mask,
            return_attn=True
        )

    # Decode tokens
    tokens = tokenizer.convert_ids_to_tokens(input_ids.cpu().numpy())

    if head is not None:
        # Single head visualization
        fig = AttentionVisualizer.plot_attention_heatmap(
            attn_weights, tokens, layer=layer, head=head
        )
    else:
        # Multi-head visualization
        fig = AttentionVisualizer.plot_multihead_attention(
            attn_weights, tokens, layer=layer
        )

    return fig

# ============================================================================
# 4. TRANSFORMER CONFIGURATIONS
# ============================================================================

TRANSFORMER_CONFIGS = {
    'base': {
        'hidden_dim': 384,
        'num_layers': 6,
        'num_heads': 8,
        'feedforward_dim': 1536,
        'dropout': 0.1,
        'activation': 'gelu'
    },
    'deep': {
        'hidden_dim': 512,
        'num_layers': 12,
        'num_heads': 16,
        'feedforward_dim': 2048,
        'dropout': 0.1,
        'activation': 'gelu'
    },
    'lite': {
        'hidden_dim': 256,
        'num_layers': 4,
        'num_heads': 8,
        'feedforward_dim': 1024,
        'dropout': 0.1,
        'activation': 'gelu'
    },
    'word2world-optimal': {
        'hidden_dim': 384,
        'num_layers': 8,
        'num_heads': 12,
        'feedforward_dim': 1536,
        'dropout': 0.1,
        'activation': 'gelu',
        'similarity_metric': 'cosine',
        'temperature': 0.05
    }
}

def create_siamese_transformer(config_name: str, vocab_size: int) -> EnhancedSiameseTransformer:
    """
    Create SiameseTransformer with predefined configuration.

    Args:
        config_name: One of 'base', 'deep', 'lite', 'word2world-optimal'
        vocab_size: Vocabulary size for embedding layer

    Returns:
        Configured EnhancedSiameseTransformer instance
    """
    if config_name not in TRANSFORMER_CONFIGS:
        raise ValueError(f"Unknown config: {config_name}. Choose from {list(TRANSFORMER_CONFIGS.keys())}")

    config = TRANSFORMER_CONFIGS[config_name]
    return EnhancedBridge2World(
        vocab_size=vocab_size,
        **{k: v for k, v in config.items() if k != 'feedforward_dim'}
    )

# ============================================================================
# 5. CONTRASTIVE PAIR DATALOADER
# ============================================================================

class BridgePairDataset(Dataset):
    """
    Dataset for contrastive learning with dynamic padding.

    Args:
        texts: List of text samples
        labels: List of corresponding labels
        tokenizer: Tokenizer function
        max_length: Maximum sequence length
        pairs_per_sample: Number of pairs to generate per sample
        random_seed: Random seed for reproducibility
    """

    def __init__(
        self,
        texts: List[str],
        labels: List[int],
        tokenizer: callable,
        max_length: int = 256,
        pairs_per_sample: int = 2,
        random_seed: int = 42
    ):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.pairs_per_sample = pairs_per_sample
        self.random_seed = random_seed

        # Validate inputs
        if len(texts) != len(labels):
            raise ValueError("Texts and labels must have same length")

        # Group samples by label
        self.label_to_indices = defaultdict(list)
        for idx, label in enumerate(labels):
            self.label_to_indices[label].append(idx)

        # Generate pairs
        self.pairs = self._generate_pairs()

        # Set random seed
        random.seed(random_seed)

    def _generate_pairs(self) -> List[Tuple[int, int, int]]:
        """
        Generate (anchor_idx, positive_idx, negative_idx) triplets.

        Returns:
            List of triplets (anchor_idx, positive_idx, negative_idx, label)
        """
        pairs = []

        for anchor_idx, anchor_label in enumerate(self.labels):
            # Get positive samples (same label)
            positive_indices = [
                idx for idx in self.label_to_indices[anchor_label]
                if idx != anchor_idx
            ]

            # Get negative samples (different labels)
            negative_labels = [
                label for label in self.label_to_indices.keys()
                if label != anchor_label
            ]

            for _ in range(self.pairs_per_sample):
                if positive_indices and negative_labels:
                    # Sample positive
                    positive_idx = random.choice(positive_indices)

                    # Sample negative label and then negative sample
                    negative_label = random.choice(negative_labels)
                    negative_idx = random.choice(self.label_to_indices[negative_label])

                    pairs.append((anchor_idx, positive_idx, negative_idx, 1))

                    # Also create a positive pair
                    pairs.append((anchor_idx, positive_idx, positive_idx, 0))

        return pairs

    def __len__(self) -> int:
        return len(self.pairs)

    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        anchor_idx, positive_idx, negative_idx, label = self.pairs[idx]

        # Get texts
        anchor_text = self.texts[anchor_idx]
        positive_text = self.texts[positive_idx]
        negative_text = self.texts[negative_idx]

        # Tokenize
        anchor_tokens = self.tokenizer(
            anchor_text,
            max_length=self.max_length,
            padding=False,
            truncation=True
        )
        positive_tokens = self.tokenizer(
            positive_text,
            max_length=self.max_length,
            padding=False,
            truncation=True
        )
        negative_tokens = self.tokenizer(
            negative_text,
            max_length=self.max_length,
            padding=False,
            truncation=True
        )

        return {
            'anchor_input_ids': anchor_tokens['input_ids'],
            'anchor_attention_mask': anchor_tokens['attention_mask'],
            'positive_input_ids': positive_tokens['input_ids'],
            'positive_attention_mask': positive_tokens['attention_mask'],
            'negative_input_ids': negative_tokens['input_ids'],
            'negative_attention_mask': negative_tokens['attention_mask'],
            'label': label
        }

def contrastive_collate_fn(batch: List[Dict]) -> Dict[str, torch.Tensor]:
    """
    Collate function for contrastive pairs with dynamic padding.

    Args:
        batch: List of batch samples

    Returns:
        Batched tensors with padding
    """
    # Separate components
    anchor_input_ids = [item['anchor_input_ids'] for item in batch]
    anchor_attention_mask = [item['anchor_attention_mask'] for item in batch]
    positive_input_ids = [item['positive_input_ids'] for item in batch]
    positive_attention_mask = [item['positive_attention_mask'] for item in batch]
    negative_input_ids = [item['negative_input_ids'] for item in batch]
    negative_attention_mask = [item['negative_attention_mask'] for item in batch]
    labels = [item['label'] for item in batch]

    # Pad sequences
    def pad_sequences(sequences):
        max_len = max(len(seq) for seq in sequences)
        padded_sequences = []
        attention_masks = []

        for seq in sequences:
            padded_seq = seq + [0] * (max_len - len(seq))
            mask = [1] * len(seq) + [0] * (max_len - len(seq))
            padded_sequences.append(padded_seq)
            attention_masks.append(mask)

        return torch.tensor(padded_sequences), torch.tensor(attention_masks)

    # Pad all sequences
    anchor_ids_padded, anchor_mask_padded = pad_sequences(anchor_input_ids)
    positive_ids_padded, positive_mask_padded = pad_sequences(positive_input_ids)
    negative_ids_padded, negative_mask_padded = pad_sequences(negative_input_ids)

    return {
        'anchor_input_ids': anchor_ids_padded,
        'anchor_attention_mask': anchor_mask_padded,
        'positive_input_ids': positive_ids_padded,
        'positive_attention_mask': positive_mask_padded,
        'negative_input_ids': negative_ids_padded,
        'negative_attention_mask': negative_mask_padded,
        'labels': torch.tensor(labels, dtype=torch.float)
    }

def create_contrastive_dataloader(
    texts: List[str],
    labels: List[int],
    tokenizer: callable,
    batch_size: int = 32,
    max_length: int = 256,
    pairs_per_sample: int = 2,
    shuffle: bool = True,
    num_workers: int = 4
) -> DataLoader:
    """
    Create DataLoader for contrastive learning.

    Args:
        texts: List of text samples
        labels: List of labels
        tokenizer: Tokenizer function
        batch_size: Batch size
        max_length: Maximum sequence length
        pairs_per_sample: Pairs per sample
        shuffle: Whether to shuffle data
        num_workers: Number of worker processes

    Returns:
        Configured DataLoader
    """
    dataset = BridgePairDataset(
        texts=texts,
        labels=labels,
        tokenizer=tokenizer,
        max_length=max_length,
        pairs_per_sample=pairs_per_sample
    )

    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        num_workers=num_workers,
        collate_fn=contrastive_collate_fn,
        pin_memory=True
    )

    return dataloader

# ============================================================================
# 6. UTILITY FUNCTIONS
# ============================================================================

def count_parameters(model: nn.Module) -> int:
    """Count total trainable parameters in model."""
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

def get_model_size(model: nn.Module) -> str:
    """Get model size in MB."""
    param_size = 0
    for param in model.parameters():
        param_size += param.nelement() * param.element_size()
    buffer_size = 0
    for buffer in model.buffers():
        buffer_size += buffer.nelement() * buffer.element_size()

    size_all_mb = (param_size + buffer_size) / 1024**2
    return f"{size_all_mb:.2f} MB"

def save_model_checkpoint(
    model: nn.Module,
    optimizer: torch.optim.Optimizer,
    epoch: int,
    loss: float,
    filepath: str
):
    """Save model checkpoint."""
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': loss,
    }, filepath)

def load_model_checkpoint(
    model: nn.Module,
    optimizer: torch.optim.Optimizer,
    filepath: str
):
    """Load model checkpoint."""
    checkpoint = torch.load(filepath)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    epoch = checkpoint['epoch']
    loss = checkpoint['loss']
    return epoch, loss
