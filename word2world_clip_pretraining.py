"""
Word2World Pre-Training Engine - CLIP Module
=============================================

CLIP fine-tuning and A/B testing framework for the Word2World platform.
Enables experimentation with different CLIP training strategies to optimize
image-text embeddings for bridge identification.

Features:
- Multiple fine-tuning strategies (standard, contrastive, triplet loss)
- A/B testing framework for comparing configurations
- Bridge-aware training optimization
- Comprehensive evaluation metrics
- Statistical significance testing

Authors: Shu Bai, Xiaoyue Cao, Jinxing Chen, Yunzhou Dai, Vieux Valcin, Huiyi Zhang
Version: 1.0
Date: October 22, 2025
License: MIT

Installation:
    pip install torch transformers pillow requests numpy scikit-learn scipy

Usage:
    from word2world_clip_pretraining import CLIPEncoderAB, CLIPExperimentConfig, CLIPABTest
    
    # Configure experiment arms
    config_a = CLIPExperimentConfig(
        arm_id="standard",
        loss_type="standard",
        learning_rate=1e-5
    )
    
    config_b = CLIPExperimentConfig(
        arm_id="contrastive",
        loss_type="contrastive",
        learning_rate=2e-5
    )
    
    # Run A/B test
    ab_test = CLIPABTest(config_a, config_b)
    results = ab_test.run_experiment(train_data, test_data)
    
    print(f"Winner: {results.winning_arm}")
    print(f"Confidence: {results.confidence:.2%}")
"""

Word2World Pre-Training Engine (CLIP Module).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SWy4KY8n3-TfFMAZa-Nh0tzqjI_bX_0r
"""

# -*- coding: utf-8 -*-
"""
CLIP A/B Testing Framework for Word2World
==========================================

A/B testing framework for evaluating different CLIP fine-tuning strategies
in the Word2World platform. Designed for use in Jupyter Notebooks and Google Colab.

Authors: Shu Bai, Xiaoyue Cao, Jinxing Chen, Yunzhou Dai, Vieux Valcin, Huiyi Zhang
Version: 1.0
Date: October 22, 2025
License: MIT

Usage in Jupyter Notebook:
    # Upload word2world.py to your notebook environment
    from word2world import STAPv4PreprocessingLayer, STAPv4Config
    from clip_ab_testing import CLIPABTest, CLIPExperimentConfig, run_clip_ab_experiment

    # Run experiment
    results = run_clip_ab_experiment(image_text_pairs, test_dataset)
"""

import torch
import torch.nn as nn
from transformers import CLIPModel, CLIPProcessor
from PIL import Image
import requests
from io import BytesIO
from typing import List, Dict, Tuple, Optional, Union
import numpy as np
from dataclasses import dataclass
import json
import logging
from sklearn.metrics.pairwise import cosine_similarity
from scipy import stats

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ============================================================================
# Configuration
# ============================================================================

@dataclass
class CLIPExperimentConfig:
    """Configuration for a CLIP experimental arm."""
    arm_id: str
    base_model: str = "openai/clip-vit-base-patch32"
    image_size: int = 224

    # Training parameters
    batch_size: int = 32
    learning_rate: float = 1e-5
    weight_decay: float = 0.01
    num_epochs: int = 10
    warmup_steps: int = 100

    # Fine-tuning strategy
    freeze_vision: bool = False
    freeze_text: bool = False
    loss_type: str = "standard"  # "standard", "contrastive", "triplet"
    margin: float = 1.0
    temperature: float = 0.07

# ============================================================================
# CLIP Encoder with A/B Testing Support
# ============================================================================

class CLIPEncoderAB:
    """
    CLIP encoder with A/B testing capabilities for different fine-tuning strategies.
    """

    def __init__(self, experiment_config: CLIPExperimentConfig, device: str = None):
        """
        Initialize CLIP encoder for A/B testing.

        Args:
            experiment_config: Experiment configuration
            device: Device for computation
        """
        self.config = experiment_config
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')

        # Initialize model and processor
        self.model = CLIPModel.from_pretrained(self.config.base_model)
        self.processor = CLIPProcessor.from_pretrained(self.config.base_model)
        self.embedding_dim = 512  # CLIP ViT-B/32

        # Apply freezing based on config
        self._apply_freezing()

        # Move to device
        self.model.to(self.device)

        # Training state
        self.is_fine_tuned = False
        self.training_history = []

        logger.info(f"CLIP encoder initialized for arm: {self.config.arm_id}")

    def _apply_freezing(self):
        """Apply parameter freezing based on configuration."""
        if self.config.freeze_vision:
            for param in self.model.vision_model.parameters():
                param.requires_grad = False
            logger.info(f"[{self.config.arm_id}] Vision encoder frozen")

        if self.config.freeze_text:
            for param in self.model.text_model.parameters():
                param.requires_grad = False
            logger.info(f"[{self.config.arm_id}] Text encoder frozen")

    def fine_tune(self, train_dataset: List[Tuple], val_dataset: List[Tuple] = None) -> Dict:
        """
        Fine-tune CLIP with the configured strategy.

        Args:
            train_dataset: List of (image, text) pairs
            val_dataset: Optional validation dataset

        Returns:
            Training results and metrics
        """
        from torch.utils.data import DataLoader, Dataset
        from tqdm import tqdm

        self.model.train()
        optimizer = torch.optim.AdamW(
            filter(lambda p: p.requires_grad, self.model.parameters()),
            lr=self.config.learning_rate,
            weight_decay=self.config.weight_decay
        )

        # Create data loader
        train_loader = self._create_data_loader(train_dataset, shuffle=True)

        # Training loop
        epoch_losses = []
        for epoch in range(self.config.num_epochs):
            total_loss = 0
            progress_bar = tqdm(train_loader, desc=f'[{self.config.arm_id}] Epoch {epoch+1}/{self.config.num_epochs}')

            for batch in progress_bar:
                # Move batch to device
                batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v
                        for k, v in batch.items()}

                # Forward pass
                outputs = self.model(**batch)

                # Compute loss based on configuration
                if self.config.loss_type == "contrastive":
                    loss = self._contrastive_loss(outputs)
                elif self.config.loss_type == "triplet":
                    loss = self._triplet_loss(outputs, margin=self.config.margin)
                else:
                    loss = self._standard_clip_loss(outputs)

                # Backward pass
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                total_loss += loss.item()
                progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})

            avg_loss = total_loss / len(train_loader)
            epoch_losses.append(avg_loss)

            # Validation if provided
            val_metrics = {}
            if val_dataset is not None:
                val_metrics = self.evaluate(val_dataset)
                logger.info(f"[{self.config.arm_id}] Epoch {epoch+1}: Loss = {avg_loss:.4f}, Val Metrics = {val_metrics}")
            else:
                logger.info(f"[{self.config.arm_id}] Epoch {epoch+1}: Loss = {avg_loss:.4f}")

        self.is_fine_tuned = True
        self.training_history.append({
            'config': self.config.__dict__,
            'losses': epoch_losses,
            'val_metrics': val_metrics
        })

        return {
            'final_loss': epoch_losses[-1],
            'val_metrics': val_metrics,
            'training_history': self.training_history
        }

    def _create_data_loader(self, dataset: List[Tuple], shuffle: bool = False):
        """Create DataLoader from image-text pairs."""
        from torch.utils.data import Dataset, DataLoader

        class CLIPDataset(Dataset):
            def __init__(self, pairs, processor):
                self.pairs = pairs
                self.processor = processor

            def __len__(self):
                return len(self.pairs)

            def __getitem__(self, idx):
                image, text = self.pairs[idx]

                # Process image
                if isinstance(image, str):
                    if image.startswith(('http://', 'https://')):
                        response = requests.get(image)
                        image = Image.open(BytesIO(response.content))
                    else:
                        image = Image.open(image)
                elif isinstance(image, np.ndarray):
                    image = Image.fromarray(image)

                # Process with CLIP processor
                inputs = self.processor(
                    text=[text],
                    images=[image],
                    return_tensors="pt",
                    padding=True
                )

                # Squeeze batch dimension
                return {k: v.squeeze(0) for k, v in inputs.items()}

        dataset_obj = CLIPDataset(dataset, self.processor)
        return DataLoader(dataset_obj, batch_size=self.config.batch_size, shuffle=shuffle)

    def _standard_clip_loss(self, outputs) -> torch.Tensor:
        """Standard CLIP contrastive loss."""
        logits_per_image = outputs.logits_per_image
        logits_per_text = outputs.logits_per_text

        # Symmetric cross entropy loss
        labels = torch.arange(logits_per_image.size(0)).to(self.device)
        loss_i = nn.functional.cross_entropy(logits_per_image / self.config.temperature, labels)
        loss_t = nn.functional.cross_entropy(logits_per_text / self.config.temperature, labels)

        return (loss_i + loss_t) / 2

    def _contrastive_loss(self, outputs) -> torch.Tensor:
        """Enhanced contrastive loss for bridge-aware training."""
        image_embeddings = outputs.image_embeds
        text_embeddings = outputs.text_embeds

        # Normalize embeddings
        image_embeddings = image_embeddings / image_embeddings.norm(dim=-1, keepdim=True)
        text_embeddings = text_embeddings / text_embeddings.norm(dim=-1, keepdim=True)

        # Compute similarity
        similarity = torch.matmul(image_embeddings, text_embeddings.T) / self.config.temperature

        # Contrastive loss with hard negative mining
        labels = torch.arange(similarity.size(0)).to(self.device)
        loss_i = nn.functional.cross_entropy(similarity, labels)
        loss_t = nn.functional.cross_entropy(similarity.T, labels)

        return (loss_i + loss_t) / 2

    def _triplet_loss(self, outputs, margin: float = 1.0) -> torch.Tensor:
        """Triplet loss for learning fine-grained relationships."""
        image_embeddings = outputs.image_embeds
        text_embeddings = outputs.text_embeds

        # Normalize
        image_embeddings = image_embeddings / image_embeddings.norm(dim=-1, keepdim=True)
        text_embeddings = text_embeddings / text_embeddings.norm(dim=-1, keepdim=True)

        # Compute distances
        positive_dist = torch.norm(image_embeddings - text_embeddings, dim=1)

        # Create negative pairs (different image-text combinations)
        batch_size = image_embeddings.size(0)
        negative_indices = torch.randperm(batch_size).to(self.device)
        negative_dist = torch.norm(image_embeddings - text_embeddings[negative_indices], dim=1)

        # Triplet loss
        loss = torch.clamp(positive_dist - negative_dist + margin, min=0.0)
        return loss.mean()

    def encode_images(self, images: List[Union[str, Image.Image, np.ndarray]]) -> np.ndarray:
        """Encode images to embeddings."""
        self.model.eval()

        if not images:
            return np.zeros((0, self.embedding_dim))

        processed_images = []
        for img in images:
            if isinstance(img, str):
                if img.startswith(('http://', 'https://')):
                    try:
                        response = requests.get(img, timeout=5)
                        img = Image.open(BytesIO(response.content))
                    except:
                        continue
                else:
                    try:
                        img = Image.open(img)
                    except:
                        continue
            elif isinstance(img, np.ndarray):
                img = Image.fromarray(img)
            processed_images.append(img)

        if not processed_images:
            return np.zeros((0, self.embedding_dim))

        with torch.no_grad():
            inputs = self.processor(images=processed_images, return_tensors="pt", padding=True)
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            outputs = self.model.get_image_features(**inputs)
            embeddings = outputs / outputs.norm(dim=-1, keepdim=True)

        return embeddings.cpu().numpy()

    def encode_texts(self, texts: List[str]) -> np.ndarray:
        """Encode texts to embeddings."""
        self.model.eval()

        if not texts:
            return np.zeros((0, self.embedding_dim))

        with torch.no_grad():
            inputs = self.processor(text=texts, return_tensors="pt", padding=True, truncation=True)
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            outputs = self.model.get_text_features(**inputs)
            embeddings = outputs / outputs.norm(dim=-1, keepdim=True)

        return embeddings.cpu().numpy()

    def evaluate(self, dataset: List[Tuple]) -> Dict[str, float]:
        """Comprehensive evaluation of CLIP model."""
        image_embeddings = self.encode_images([pair[0] for pair in dataset])
        text_embeddings = self.encode_texts([pair[1] for pair in dataset])

        similarity_matrix = cosine_similarity(image_embeddings, text_embeddings)

        metrics = {
            'image_text_alignment': np.mean(np.diag(similarity_matrix)),
            'alignment_consistency': self._compute_alignment_consistency(similarity_matrix),
            'retrieval_accuracy@1': self._compute_retrieval_accuracy(similarity_matrix, k=1),
            'retrieval_accuracy@5': self._compute_retrieval_accuracy(similarity_matrix, k=5),
            'semantic_coherence': np.mean(cosine_similarity(text_embeddings)),
            'visual_diversity': np.std(image_embeddings, axis=0).mean()
        }

        return metrics

    def _compute_alignment_consistency(self, similarity_matrix: np.ndarray) -> float:
        """Compute how consistently images and texts align."""
        diagonal = np.diag(similarity_matrix)
        off_diagonal = (np.sum(similarity_matrix) - np.sum(diagonal)) / (similarity_matrix.size - len(diagonal))
        return np.mean(diagonal) - off_diagonal

    def _compute_retrieval_accuracy(self, similarity_matrix: np.ndarray, k: int = 1) -> float:
        """Compute retrieval accuracy@k."""
        n = similarity_matrix.shape[0]
        correct = 0

        for i in range(n):
            # Image to text retrieval
            image_scores = similarity_matrix[i]
            top_k_indices = np.argsort(image_scores)[-k:][::-1]
            if i in top_k_indices:
                correct += 1

            # Text to image retrieval
            text_scores = similarity_matrix[:, i]
            top_k_indices = np.argsort(text_scores)[-k:][::-1]
            if i in top_k_indices:
                correct += 1

        return correct / (2 * n)

# ============================================================================
# A/B Testing Framework
# ============================================================================

class CLIPABTest:
    """
    A/B testing framework for comparing CLIP configurations.
    """

    def __init__(self, arm_a_config: CLIPExperimentConfig, arm_b_config: CLIPExperimentConfig):
        """
        Initialize A/B test with two experimental arms.

        Args:
            arm_a_config: Configuration for arm A
            arm_b_config: Configuration for arm B
        """
        self.arm_a_config = arm_a_config
        self.arm_b_config = arm_b_config

        self.arm_a_encoder = CLIPEncoderAB(arm_a_config)
        self.arm_b_encoder = CLIPEncoderAB(arm_b_config)

        self.results = None

        logger.info(f"CLIP A/B Test initialized: {arm_a_config.arm_id} vs {arm_b_config.arm_id}")

    def run_experiment(self, train_dataset: List[Tuple], test_dataset: List[Tuple]) -> Dict:
        """
        Run the complete A/B experiment.

        Args:
            train_dataset: Training data (image, text) pairs
            test_dataset: Test data for evaluation

        Returns:
            Comprehensive experiment results
        """
        logger.info("Starting CLIP A/B experiment...")

        # Train both arms
        logger.info(f"Training Arm A: {self.arm_a_config.arm_id}")
        arm_a_train_results = self.arm_a_encoder.fine_tune(train_dataset)

        logger.info(f"Training Arm B: {self.arm_b_config.arm_id}")
        arm_b_train_results = self.arm_b_encoder.fine_tune(train_dataset)

        # Evaluate both arms
        logger.info("Evaluating both arms on test set...")
        arm_a_metrics = self.arm_a_encoder.evaluate(test_dataset)
        arm_b_metrics = self.arm_b_encoder.evaluate(test_dataset)

        # Statistical analysis
        statistical_tests = self._perform_statistical_analysis(arm_a_metrics, arm_b_metrics)

        # Determine winner
        winning_arm, confidence = self._select_winning_arm(arm_a_metrics, arm_b_metrics, statistical_tests)

        # Compile results
        self.results = {
            'arm_a': {
                'config': self.arm_a_config.__dict__,
                'train_results': arm_a_train_results,
                'test_metrics': arm_a_metrics
            },
            'arm_b': {
                'config': self.arm_b_config.__dict__,
                'train_results': arm_b_train_results,
                'test_metrics': arm_b_metrics
            },
            'statistical_tests': statistical_tests,
            'winning_arm': winning_arm,
            'confidence': confidence
        }

        logger.info(f"Experiment completed. Winner: {winning_arm} (confidence: {confidence:.3f})")
        return self.results

    def _perform_statistical_analysis(self, arm_a_metrics: Dict, arm_b_metrics: Dict) -> Dict:
        """Perform statistical significance tests."""
        tests = {}

        for metric_name in arm_a_metrics.keys():
            a_val = arm_a_metrics[metric_name]
            b_val = arm_b_metrics[metric_name]

            # Simple comparison (in production, use proper statistical tests with multiple samples)
            diff = abs(a_val - b_val)
            relative_diff = diff / max(abs(a_val), abs(b_val), 1e-10)

            tests[metric_name] = {
                'arm_a_value': a_val,
                'arm_b_value': b_val,
                'difference': a_val - b_val,
                'relative_difference': relative_diff,
                'significant': relative_diff > 0.05  # 5% threshold
            }

        return tests

    def _select_winning_arm(self, arm_a_metrics: Dict, arm_b_metrics: Dict,
                          statistical_tests: Dict) -> Tuple[str, float]:
        """Select winning arm based on metrics."""
        # Primary metric: image_text_alignment
        primary_metric = 'image_text_alignment'

        a_score = arm_a_metrics[primary_metric]
        b_score = arm_b_metrics[primary_metric]

        if a_score > b_score:
            winning_arm = self.arm_a_config.arm_id
            confidence = (a_score - b_score) / max(a_score, b_score)
        else:
            winning_arm = self.arm_b_config.arm_id
            confidence = (b_score - a_score) / max(a_score, b_score)

        # Adjust confidence based on statistical significance
        if statistical_tests[primary_metric]['significant']:
            confidence *= 1.2

        confidence = min(confidence, 1.0)

        return winning_arm, confidence

    def generate_report(self) -> str:
        """Generate comprehensive experiment report."""
        if self.results is None:
            return "No experiment results available."

        report = []
        report.append("=" * 80)
        report.append("CLIP A/B EXPERIMENT REPORT")
        report.append("=" * 80)

        # Configuration summary
        report.append("\nEXPERIMENT CONFIGURATION:")
        report.append("-" * 40)
        report.append(f"Arm A: {self.arm_a_config.arm_id}")
        report.append(f"  Loss Type: {self.arm_a_config.loss_type}")
        report.append(f"  Freeze Vision: {self.arm_a_config.freeze_vision}")
        report.append(f"  Freeze Text: {self.arm_a_config.freeze_text}")
        report.append(f"\nArm B: {self.arm_b_config.arm_id}")
        report.append(f"  Loss Type: {self.arm_b_config.loss_type}")
        report.append(f"  Freeze Vision: {self.arm_b_config.freeze_vision}")
        report.append(f"  Freeze Text: {self.arm_b_config.freeze_text}")

        # Results summary
        report.append("\nEXPERIMENT RESULTS:")
        report.append("-" * 40)
        report.append(f"Winning Arm: {self.results['winning_arm']}")
        report.append(f"Confidence: {self.results['confidence']:.3f}")

        # Metrics comparison
        report.append("\nMETRICS COMPARISON:")
        report.append("-" * 40)
        for metric_name, test_info in self.results['statistical_tests'].items():
            a_val = test_info['arm_a_value']
            b_val = test_info['arm_b_value']
            diff = test_info['difference']
            sig_marker = "***" if test_info['significant'] else ""

            report.append(f"{metric_name}:")
            report.append(f"  Arm A: {a_val:.4f}")
            report.append(f"  Arm B: {b_val:.4f}")
            report.append(f"  Difference: {diff:+.4f} {sig_marker}")

        # Recommendations
        report.append("\nRECOMMENDATIONS:")
        report.append("-" * 40)
        if self.results['confidence'] > 0.7:
            report.append(f"âœ… STRONG EVIDENCE: Deploy {self.results['winning_arm']} configuration")
        elif self.results['confidence'] > 0.5:
            report.append(f"ðŸ“Š MODERATE EVIDENCE: Consider {self.results['winning_arm']} configuration")
        else:
            report.append("ðŸ” INCONCLUSIVE: Continue experimentation or refine configurations")

        return "\n".join(report)

# ============================================================================
# Quick Start Function
# ============================================================================

def run_clip_ab_experiment(
    train_dataset: List[Tuple],
    test_dataset: List[Tuple],
    arm_a_config: Optional[CLIPExperimentConfig] = None,
    arm_b_config: Optional[CLIPExperimentConfig] = None
) -> Dict:
    """
    Quick start function to run CLIP A/B experiment.

    Args:
        train_dataset: Training data (image, text) pairs
        test_dataset: Test data for evaluation
        arm_a_config: Optional configuration for arm A (uses default if None)
        arm_b_config: Optional configuration for arm B (uses default if None)

    Returns:
        Experiment results dictionary
    """
    # Default configurations
    if arm_a_config is None:
        arm_a_config = CLIPExperimentConfig(
            arm_id="baseline",
            loss_type="standard",
            freeze_vision=False,
            freeze_text=False,
            num_epochs=5
        )

    if arm_b_config is None:
        arm_b_config = CLIPExperimentConfig(
            arm_id="contrastive",
            loss_type="contrastive",
            freeze_vision=False,
            freeze_text=False,
            num_epochs=5
        )

    # Run experiment
    ab_test = CLIPABTest(arm_a_config, arm_b_config)
    results = ab_test.run_experiment(train_dataset, test_dataset)

    # Print report
    print(ab_test.generate_report())

    return results

# ============================================================================
# Example Usage (for Jupyter Notebook)
# ============================================================================

if __name__ == "__main__":
    print("CLIP A/B Testing Framework")
    print("=" * 80)
    print("\nExample usage in Jupyter Notebook:")
    print("""
    # 1. Prepare your data
    train_data = [
        (image1, "description of image 1"),
        (image2, "description of image 2"),
        # ... more pairs
    ]

    test_data = [
        (test_image1, "description of test image 1"),
        # ... more test pairs
    ]

    # 2. Run A/B experiment
    from clip_ab_testing import run_clip_ab_experiment, CLIPExperimentConfig

    results = run_clip_ab_experiment(train_data, test_data)

    # 3. Or customize configurations
    arm_a = CLIPExperimentConfig(
        arm_id="baseline",
        loss_type="standard",
        num_epochs=10
    )

    arm_b = CLIPExperimentConfig(
        arm_id="triplet",
        loss_type="triplet",
        margin=1.5,
        num_epochs=10
    )

    results = run_clip_ab_experiment(train_data, test_data, arm_a, arm_b)
    """)